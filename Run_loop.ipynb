{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/clusterdata/uqmziaei/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The environment has the following 4 actions: ['NOOP', 'FIRE', 'RIGHT', 'LEFT']\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 84, 84, 4)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "norm (Lambda)                   (None, 84, 84, 4)    0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 20, 20, 32)   8224        norm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 9, 9, 64)     32832       conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 7, 7, 64)     36928       conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 3136)         0           conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 512)          1606144     flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 4)            2052        dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "action_mask (InputLayer)        (None, 4)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "QValue (Multiply)               (None, 4)            0           dense_4[0][0]                    \n",
      "                                                                 action_mask[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 1,686,180\n",
      "Trainable params: 1,686,180\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 84, 84, 4)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "norm (Lambda)                   (None, 84, 84, 4)    0           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 20, 20, 32)   8224        norm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 9, 9, 64)     32832       conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 7, 7, 64)     36928       conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 3136)         0           conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 512)          1606144     flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 4)            2052        dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "action_mask (InputLayer)        (None, 4)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "QValue (Multiply)               (None, 4)            0           dense_6[0][0]                    \n",
      "                                                                 action_mask[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 1,686,180\n",
      "Trainable params: 1,686,180\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "MAX_EPISODE_LENGTH :  18000\n",
      "NO_OP_STEPS :  10\n",
      "MAX_EPISODES :  2000\n",
      "AGENT_HISTORY_LENGTH :  4\n",
      "UPDATE_FREQ :  4\n",
      "NETW_UPDATE_FREQ :  10000\n",
      "REPLAY_MEMORY_START_SIZE :  50000\n",
      "DISCOUNT_FACTOR :  0.99\n",
      "MEMORY_SIZE :  1000000\n",
      "BS :  32\n",
      "LEARNING_RATE :  0.0001\n",
      "PUNISH :  1.0\n",
      "INI_EPSILON :  1.0\n",
      "END_EPSILON :  0.1\n",
      "MIN_OBSERVE_EPISODE :  200\n",
      "GAME_ENV :  BreakoutDeterministic-v4\n",
      "REWARD_EXTRAPOLATION_EXPONENT :  -10.0\n",
      "LINEAR_EXPLORATION_EXPONENT :  True\n",
      "USE_DOUBLE_MODEL :  True\n",
      "frame_height :  84\n",
      "frame_width :  84\n",
      "****************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/clusterdata/uqmziaei/anaconda3/lib/python3.6/site-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n",
      "/clusterdata/uqmziaei/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/clusterdata/uqmziaei/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'time': '0:00:00.269540', 'episode': 0, 'total_frames': 125.0, 'epsilon': '1.000', 'highest_reward': 0, 'mean_rewards': 0.0, 'mean_loss': 'nan', 'estimating_reward': False, 'reward_exponent': 1.0}\n",
      "{'time': '0:00:04.126378', 'episode': 10, 'total_frames': 1975.0, 'epsilon': '1.000', 'highest_reward': 8.0, 'mean_rewards': 1.7, 'mean_loss': 'nan', 'estimating_reward': False, 'reward_exponent': 1.0}\n",
      "{'time': '0:00:07.303785', 'episode': 20, 'total_frames': 3494.0, 'epsilon': '1.000', 'highest_reward': 8.0, 'mean_rewards': 0.4, 'mean_loss': 'nan', 'estimating_reward': False, 'reward_exponent': 1.0}\n",
      "{'time': '0:00:11.749363', 'episode': 30, 'total_frames': 5639.0, 'epsilon': '1.000', 'highest_reward': 8.0, 'mean_rewards': 2.2, 'mean_loss': 'nan', 'estimating_reward': False, 'reward_exponent': 1.0}\n",
      "{'time': '0:00:15.257809', 'episode': 40, 'total_frames': 7301.0, 'epsilon': '1.000', 'highest_reward': 8.0, 'mean_rewards': 0.8, 'mean_loss': 'nan', 'estimating_reward': False, 'reward_exponent': 1.0}\n",
      "{'time': '0:00:18.874035', 'episode': 50, 'total_frames': 9036.0, 'epsilon': '1.000', 'highest_reward': 8.0, 'mean_rewards': 1.0, 'mean_loss': 'nan', 'estimating_reward': False, 'reward_exponent': 1.0}\n",
      "{'time': '0:00:23.041974', 'episode': 60, 'total_frames': 11040.0, 'epsilon': '1.000', 'highest_reward': 8.0, 'mean_rewards': 1.9, 'mean_loss': 'nan', 'estimating_reward': False, 'reward_exponent': 1.0}\n",
      "{'time': '0:00:26.307881', 'episode': 70, 'total_frames': 12590.0, 'epsilon': '1.000', 'highest_reward': 8.0, 'mean_rewards': 0.5, 'mean_loss': 'nan', 'estimating_reward': False, 'reward_exponent': 1.0}\n",
      "{'time': '0:00:29.263474', 'episode': 80, 'total_frames': 14000.0, 'epsilon': '1.000', 'highest_reward': 8.0, 'mean_rewards': 0.2, 'mean_loss': 'nan', 'estimating_reward': False, 'reward_exponent': 1.0}\n",
      "{'time': '0:00:32.707546', 'episode': 90, 'total_frames': 15641.0, 'epsilon': '1.000', 'highest_reward': 8.0, 'mean_rewards': 0.8, 'mean_loss': 'nan', 'estimating_reward': False, 'reward_exponent': 1.0}\n",
      "{'time': '0:00:36.431381', 'episode': 100, 'total_frames': 17413.0, 'epsilon': '1.000', 'highest_reward': 8.0, 'mean_rewards': 1.1, 'mean_loss': 'nan', 'estimating_reward': False, 'reward_exponent': 1.0}\n",
      "{'time': '0:00:39.843622', 'episode': 110, 'total_frames': 19044.0, 'epsilon': '1.000', 'highest_reward': 8.0, 'mean_rewards': 0.9, 'mean_loss': 'nan', 'estimating_reward': False, 'reward_exponent': 1.0}\n",
      "{'time': '0:00:44.236981', 'episode': 120, 'total_frames': 21159.0, 'epsilon': '1.000', 'highest_reward': 8.0, 'mean_rewards': 2.2, 'mean_loss': 'nan', 'estimating_reward': False, 'reward_exponent': 1.0}\n",
      "{'time': '0:00:47.562951', 'episode': 130, 'total_frames': 22740.0, 'epsilon': '1.000', 'highest_reward': 8.0, 'mean_rewards': 0.7, 'mean_loss': 'nan', 'estimating_reward': False, 'reward_exponent': 1.0}\n",
      "{'time': '0:00:51.414498', 'episode': 140, 'total_frames': 24575.0, 'epsilon': '1.000', 'highest_reward': 8.0, 'mean_rewards': 1.3, 'mean_loss': 'nan', 'estimating_reward': False, 'reward_exponent': 1.0}\n",
      "{'time': '0:00:55.492902', 'episode': 150, 'total_frames': 26530.0, 'epsilon': '1.000', 'highest_reward': 8.0, 'mean_rewards': 1.7, 'mean_loss': 'nan', 'estimating_reward': False, 'reward_exponent': 1.0}\n",
      "{'time': '0:00:58.582948', 'episode': 160, 'total_frames': 27988.0, 'epsilon': '1.000', 'highest_reward': 8.0, 'mean_rewards': 0.4, 'mean_loss': 'nan', 'estimating_reward': False, 'reward_exponent': 1.0}\n",
      "{'time': '0:01:02.158481', 'episode': 170, 'total_frames': 29670.0, 'epsilon': '1.000', 'highest_reward': 8.0, 'mean_rewards': 1.0, 'mean_loss': 'nan', 'estimating_reward': False, 'reward_exponent': 1.0}\n",
      "{'time': '0:01:05.880312', 'episode': 180, 'total_frames': 31449.0, 'epsilon': '1.000', 'highest_reward': 8.0, 'mean_rewards': 1.1, 'mean_loss': 'nan', 'estimating_reward': False, 'reward_exponent': 1.0}\n",
      "{'time': '0:01:09.176103', 'episode': 190, 'total_frames': 33027.0, 'epsilon': '1.000', 'highest_reward': 8.0, 'mean_rewards': 0.8, 'mean_loss': 'nan', 'estimating_reward': False, 'reward_exponent': 1.0}\n",
      "{'time': '0:01:13.242082', 'episode': 200, 'total_frames': 34987.0, 'epsilon': '1.000', 'highest_reward': 8.0, 'mean_rewards': 1.9, 'mean_loss': 'nan', 'estimating_reward': False, 'reward_exponent': 1.0}\n",
      "{'time': '0:01:16.782067', 'episode': 210, 'total_frames': 36682.0, 'epsilon': '1.000', 'highest_reward': 8.0, 'mean_rewards': 1.0, 'mean_loss': 'nan', 'estimating_reward': False, 'reward_exponent': 1.0}\n",
      "{'time': '0:01:20.017942', 'episode': 220, 'total_frames': 38230.0, 'epsilon': '1.000', 'highest_reward': 8.0, 'mean_rewards': 0.6, 'mean_loss': 'nan', 'estimating_reward': False, 'reward_exponent': 1.0}\n",
      "{'time': '0:01:23.480278', 'episode': 230, 'total_frames': 39883.0, 'epsilon': '1.000', 'highest_reward': 8.0, 'mean_rewards': 0.9, 'mean_loss': 'nan', 'estimating_reward': False, 'reward_exponent': 1.0}\n",
      "{'time': '0:01:26.796300', 'episode': 240, 'total_frames': 41463.0, 'epsilon': '1.000', 'highest_reward': 8.0, 'mean_rewards': 0.6, 'mean_loss': 'nan', 'estimating_reward': False, 'reward_exponent': 1.0}\n",
      "{'time': '0:01:30.776191', 'episode': 250, 'total_frames': 43378.0, 'epsilon': '1.000', 'highest_reward': 8.0, 'mean_rewards': 1.6, 'mean_loss': 'nan', 'estimating_reward': False, 'reward_exponent': 1.0}\n",
      "{'time': '0:01:34.756857', 'episode': 260, 'total_frames': 45294.0, 'epsilon': '1.000', 'highest_reward': 8.0, 'mean_rewards': 1.5, 'mean_loss': 'nan', 'estimating_reward': False, 'reward_exponent': 1.0}\n",
      "{'time': '0:01:38.706348', 'episode': 270, 'total_frames': 47184.0, 'epsilon': '1.000', 'highest_reward': 8.0, 'mean_rewards': 1.4, 'mean_loss': 'nan', 'estimating_reward': False, 'reward_exponent': 1.0}\n",
      "{'time': '0:01:42.373217', 'episode': 280, 'total_frames': 48947.0, 'epsilon': '1.000', 'highest_reward': 8.0, 'mean_rewards': 1.2, 'mean_loss': 'nan', 'estimating_reward': False, 'reward_exponent': 1.0}\n",
      "{'time': '0:01:56.157169', 'episode': 290, 'total_frames': 50564.0, 'epsilon': '0.994', 'highest_reward': 8.0, 'mean_rewards': 0.8, 'mean_loss': '0.00507', 'estimating_reward': False, 'reward_exponent': 1.0}\n",
      "{'time': '0:02:27.986395', 'episode': 300, 'total_frames': 52225.0, 'epsilon': '0.978', 'highest_reward': 8.0, 'mean_rewards': 1.0, 'mean_loss': '0.00578', 'estimating_reward': False, 'reward_exponent': 1.0}\n",
      "{'time': '0:03:04.705614', 'episode': 310, 'total_frames': 54130.0, 'epsilon': '0.959', 'highest_reward': 8.0, 'mean_rewards': 1.5, 'mean_loss': '0.00369', 'estimating_reward': False, 'reward_exponent': 1.0}\n",
      "{'time': '0:03:38.016094', 'episode': 320, 'total_frames': 55853.0, 'epsilon': '0.941', 'highest_reward': 8.0, 'mean_rewards': 1.2, 'mean_loss': '0.00351', 'estimating_reward': False, 'reward_exponent': 1.0}\n",
      "{'time': '0:04:11.478038', 'episode': 330, 'total_frames': 57591.0, 'epsilon': '0.924', 'highest_reward': 8.0, 'mean_rewards': 1.1, 'mean_loss': '0.00413', 'estimating_reward': False, 'reward_exponent': 1.0}\n",
      "{'time': '0:04:46.340360', 'episode': 340, 'total_frames': 59377.0, 'epsilon': '0.906', 'highest_reward': 8.0, 'mean_rewards': 1.2, 'mean_loss': '0.00351', 'estimating_reward': False, 'reward_exponent': 1.0}\n",
      "Updating the target network\n",
      "{'time': '0:05:23.426506', 'episode': 350, 'total_frames': 61286.0, 'epsilon': '0.887', 'highest_reward': 8.0, 'mean_rewards': 1.5, 'mean_loss': '0.00071', 'estimating_reward': False, 'reward_exponent': 1.0}\n",
      "{'time': '0:05:57.702051', 'episode': 360, 'total_frames': 63006.0, 'epsilon': '0.870', 'highest_reward': 8.0, 'mean_rewards': 1.1, 'mean_loss': '0.00303', 'estimating_reward': False, 'reward_exponent': 1.0}\n",
      "{'time': '0:06:31.682018', 'episode': 370, 'total_frames': 64700.0, 'epsilon': '0.853', 'highest_reward': 8.0, 'mean_rewards': 1.0, 'mean_loss': '0.00172', 'estimating_reward': False, 'reward_exponent': 1.0}\n",
      "{'time': '0:07:14.191257', 'episode': 380, 'total_frames': 66837.0, 'epsilon': '0.832', 'highest_reward': 8.0, 'mean_rewards': 2.2, 'mean_loss': '0.00214', 'estimating_reward': False, 'reward_exponent': 1.0}\n",
      "{'time': '0:07:53.993242', 'episode': 390, 'total_frames': 68835.0, 'epsilon': '0.812', 'highest_reward': 8.0, 'mean_rewards': 1.8, 'mean_loss': '0.00185', 'estimating_reward': False, 'reward_exponent': 1.0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating the target network\n",
      "{'time': '0:08:32.582710', 'episode': 400, 'total_frames': 70743.0, 'epsilon': '0.793', 'highest_reward': 8.0, 'mean_rewards': 1.4, 'mean_loss': '0.00245', 'estimating_reward': False, 'reward_exponent': 1.0}\n",
      "{'time': '0:09:04.035005', 'episode': 410, 'total_frames': 72319.0, 'epsilon': '0.777', 'highest_reward': 8.0, 'mean_rewards': 0.8, 'mean_loss': '0.00161', 'estimating_reward': False, 'reward_exponent': 1.2785714285714285}\n",
      "{'time': '0:09:41.901766', 'episode': 420, 'total_frames': 74195.0, 'epsilon': '0.758', 'highest_reward': 8.0, 'mean_rewards': 1.5, 'mean_loss': '0.00233', 'estimating_reward': False, 'reward_exponent': 1.5571428571428572}\n",
      "{'time': '0:10:14.747048', 'episode': 430, 'total_frames': 75810.0, 'epsilon': '0.742', 'highest_reward': 8.0, 'mean_rewards': 0.9, 'mean_loss': '0.00148', 'estimating_reward': False, 'reward_exponent': 1.8357142857142859}\n",
      "{'time': '0:10:53.399419', 'episode': 440, 'total_frames': 77695.0, 'epsilon': '0.723', 'highest_reward': 8.0, 'mean_rewards': 1.6, 'mean_loss': '0.00204', 'estimating_reward': False, 'reward_exponent': 2.1142857142857143}\n",
      "{'time': '0:11:38.204238', 'episode': 450, 'total_frames': 79842.0, 'epsilon': '0.702', 'highest_reward': 8.0, 'mean_rewards': 2.2, 'mean_loss': '0.00217', 'estimating_reward': False, 'reward_exponent': 2.392857142857143}\n",
      "Updating the target network\n",
      "{'time': '0:12:12.103415', 'episode': 460, 'total_frames': 81459.0, 'epsilon': '0.685', 'highest_reward': 8.0, 'mean_rewards': 1.0, 'mean_loss': '0.00146', 'estimating_reward': False, 'reward_exponent': 2.6714285714285717}\n",
      "{'time': '0:12:50.792109', 'episode': 470, 'total_frames': 83297.0, 'epsilon': '0.667', 'highest_reward': 8.0, 'mean_rewards': 1.3, 'mean_loss': '0.00128', 'estimating_reward': False, 'reward_exponent': 2.95}\n",
      "{'time': '0:13:29.420345', 'episode': 480, 'total_frames': 85158.0, 'epsilon': '0.648', 'highest_reward': 8.0, 'mean_rewards': 1.6, 'mean_loss': '0.00097', 'estimating_reward': False, 'reward_exponent': 3.2285714285714286}\n",
      "{'time': '0:14:07.601642', 'episode': 490, 'total_frames': 86949.0, 'epsilon': '0.631', 'highest_reward': 8.0, 'mean_rewards': 1.5, 'mean_loss': '0.00146', 'estimating_reward': False, 'reward_exponent': 3.507142857142857}\n",
      "{'time': '0:14:47.180360', 'episode': 500, 'total_frames': 88803.0, 'epsilon': '0.612', 'highest_reward': 8.0, 'mean_rewards': 1.6, 'mean_loss': '0.00116', 'estimating_reward': False, 'reward_exponent': 3.7857142857142856}\n",
      "Updating the target network\n",
      "{'time': '0:15:24.418499', 'episode': 510, 'total_frames': 90560.0, 'epsilon': '0.594', 'highest_reward': 8.0, 'mean_rewards': 1.3, 'mean_loss': '0.00135', 'estimating_reward': False, 'reward_exponent': 4.064285714285715}\n",
      "{'time': '0:16:06.481331', 'episode': 520, 'total_frames': 92521.0, 'epsilon': '0.575', 'highest_reward': 8.0, 'mean_rewards': 1.8, 'mean_loss': '0.00067', 'estimating_reward': False, 'reward_exponent': 4.342857142857143}\n",
      "{'time': '0:16:45.554902', 'episode': 530, 'total_frames': 94324.0, 'epsilon': '0.557', 'highest_reward': 8.0, 'mean_rewards': 1.6, 'mean_loss': '0.00112', 'estimating_reward': False, 'reward_exponent': 4.621428571428572}\n",
      "{'time': '0:17:25.641186', 'episode': 540, 'total_frames': 96153.0, 'epsilon': '0.538', 'highest_reward': 8.0, 'mean_rewards': 1.5, 'mean_loss': '0.00116', 'estimating_reward': False, 'reward_exponent': 4.9}\n",
      "{'time': '0:18:03.890380', 'episode': 550, 'total_frames': 97883.0, 'epsilon': '0.521', 'highest_reward': 8.0, 'mean_rewards': 1.2, 'mean_loss': '0.00102', 'estimating_reward': False, 'reward_exponent': 5.178571428571429}\n",
      "{'time': '0:18:39.348071', 'episode': 560, 'total_frames': 99474.0, 'epsilon': '0.505', 'highest_reward': 8.0, 'mean_rewards': 0.9, 'mean_loss': '0.00163', 'estimating_reward': False, 'reward_exponent': 5.457142857142857}\n",
      "Updating the target network\n",
      "{'time': '0:19:27.309851', 'episode': 570, 'total_frames': 101650.0, 'epsilon': '0.484', 'highest_reward': 8.0, 'mean_rewards': 2.4, 'mean_loss': '0.00098', 'estimating_reward': False, 'reward_exponent': 5.735714285714286}\n",
      "{'time': '0:20:11.611588', 'episode': 580, 'total_frames': 103648.0, 'epsilon': '0.464', 'highest_reward': 8.0, 'mean_rewards': 2.0, 'mean_loss': '0.00103', 'estimating_reward': False, 'reward_exponent': 6.014285714285714}\n",
      "{'time': '0:20:57.568558', 'episode': 590, 'total_frames': 105687.0, 'epsilon': '0.443', 'highest_reward': 8.0, 'mean_rewards': 2.0, 'mean_loss': '0.00105', 'estimating_reward': False, 'reward_exponent': 6.292857142857143}\n",
      "{'time': '0:21:37.677215', 'episode': 600, 'total_frames': 107446.0, 'epsilon': '0.426', 'highest_reward': 8.0, 'mean_rewards': 1.3, 'mean_loss': '0.00054', 'estimating_reward': False, 'reward_exponent': 6.571428571428571}\n",
      "{'time': '0:22:17.615653', 'episode': 610, 'total_frames': 109219.0, 'epsilon': '0.408', 'highest_reward': 8.0, 'mean_rewards': 1.6, 'mean_loss': '0.00085', 'estimating_reward': False, 'reward_exponent': 6.8500000000000005}\n",
      "Updating the target network\n",
      "{'time': '0:23:05.295795', 'episode': 620, 'total_frames': 111311.0, 'epsilon': '0.387', 'highest_reward': 8.0, 'mean_rewards': 2.2, 'mean_loss': '0.00083', 'estimating_reward': False, 'reward_exponent': 7.128571428571429}\n",
      "{'time': '0:23:46.153947', 'episode': 630, 'total_frames': 113090.0, 'epsilon': '0.369', 'highest_reward': 8.0, 'mean_rewards': 1.4, 'mean_loss': '0.00086', 'estimating_reward': False, 'reward_exponent': 7.4071428571428575}\n",
      "{'time': '0:24:40.897036', 'episode': 640, 'total_frames': 115465.0, 'epsilon': '0.345', 'highest_reward': 8.0, 'mean_rewards': 2.7, 'mean_loss': '0.00119', 'estimating_reward': False, 'reward_exponent': 7.685714285714286}\n",
      "{'time': '0:25:20.215675', 'episode': 650, 'total_frames': 117186.0, 'epsilon': '0.328', 'highest_reward': 8.0, 'mean_rewards': 1.2, 'mean_loss': '0.00057', 'estimating_reward': False, 'reward_exponent': 7.964285714285714}\n",
      "{'time': '0:26:03.753746', 'episode': 660, 'total_frames': 119080.0, 'epsilon': '0.309', 'highest_reward': 8.0, 'mean_rewards': 1.6, 'mean_loss': '0.00081', 'estimating_reward': False, 'reward_exponent': 8.242857142857144}\n",
      "Updating the target network\n",
      "{'time': '0:26:56.378003', 'episode': 670, 'total_frames': 121340.0, 'epsilon': '0.287', 'highest_reward': 11.0, 'mean_rewards': 2.9, 'mean_loss': '0.00227', 'estimating_reward': False, 'reward_exponent': 8.521428571428572}\n",
      "{'time': '0:27:50.433108', 'episode': 680, 'total_frames': 123637.0, 'epsilon': '0.264', 'highest_reward': 11.0, 'mean_rewards': 2.8, 'mean_loss': '0.00095', 'estimating_reward': False, 'reward_exponent': 8.8}\n",
      "{'time': '0:28:37.581401', 'episode': 690, 'total_frames': 125622.0, 'epsilon': '0.244', 'highest_reward': 11.0, 'mean_rewards': 1.9, 'mean_loss': '0.00079', 'estimating_reward': False, 'reward_exponent': 9.07857142857143}\n",
      "{'time': '0:29:23.934680', 'episode': 700, 'total_frames': 127570.0, 'epsilon': '0.224', 'highest_reward': 11.0, 'mean_rewards': 1.9, 'mean_loss': '0.00106', 'estimating_reward': False, 'reward_exponent': 9.357142857142858}\n",
      "{'time': '0:30:11.578018', 'episode': 710, 'total_frames': 129567.0, 'epsilon': '0.204', 'highest_reward': 11.0, 'mean_rewards': 1.9, 'mean_loss': '0.00060', 'estimating_reward': False, 'reward_exponent': 9.635714285714286}\n",
      "Updating the target network\n",
      "{'time': '0:31:03.710826', 'episode': 720, 'total_frames': 131732.0, 'epsilon': '0.183', 'highest_reward': 11.0, 'mean_rewards': 2.2, 'mean_loss': '0.00086', 'estimating_reward': False, 'reward_exponent': 9.914285714285715}\n",
      "{'time': '0:31:48.185251', 'episode': 730, 'total_frames': 133575.0, 'epsilon': '0.164', 'highest_reward': 11.0, 'mean_rewards': 2.0, 'mean_loss': '0.00079', 'estimating_reward': False, 'reward_exponent': 10.192857142857143}\n",
      "{'time': '0:32:42.021101', 'episode': 740, 'total_frames': 135809.0, 'epsilon': '0.142', 'highest_reward': 11.0, 'mean_rewards': 2.8, 'mean_loss': '0.00120', 'estimating_reward': False, 'reward_exponent': 10.471428571428572}\n",
      "{'time': '0:33:28.017874', 'episode': 750, 'total_frames': 137700.0, 'epsilon': '0.123', 'highest_reward': 11.0, 'mean_rewards': 1.8, 'mean_loss': '0.00060', 'estimating_reward': False, 'reward_exponent': 10.75}\n",
      "{'time': '0:34:18.467538', 'episode': 760, 'total_frames': 139777.0, 'epsilon': '0.102', 'highest_reward': 11.0, 'mean_rewards': 2.2, 'mean_loss': '0.00082', 'estimating_reward': False, 'reward_exponent': 11.028571428571428}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating the target network\n",
      "{'time': '0:35:14.258819', 'episode': 770, 'total_frames': 142057.0, 'epsilon': '0.100', 'highest_reward': 11.0, 'mean_rewards': 2.9, 'mean_loss': '0.00100', 'estimating_reward': False, 'reward_exponent': 11.307142857142857}\n",
      "{'time': '0:36:04.339276', 'episode': 780, 'total_frames': 144122.0, 'epsilon': '0.100', 'highest_reward': 11.0, 'mean_rewards': 2.1, 'mean_loss': '0.00218', 'estimating_reward': False, 'reward_exponent': 11.585714285714285}\n",
      "{'time': '0:37:13.429023', 'episode': 790, 'total_frames': 146948.0, 'epsilon': '0.100', 'highest_reward': 11.0, 'mean_rewards': 4.9, 'mean_loss': '0.00079', 'estimating_reward': False, 'reward_exponent': 11.864285714285714}\n",
      "{'time': '0:38:11.496807', 'episode': 800, 'total_frames': 149305.0, 'epsilon': '0.100', 'highest_reward': 11.0, 'mean_rewards': 2.9, 'mean_loss': '0.00037', 'estimating_reward': False, 'reward_exponent': 12.142857142857142}\n",
      "Updating the target network\n",
      "{'time': '0:39:08.470404', 'episode': 810, 'total_frames': 151630.0, 'epsilon': '0.100', 'highest_reward': 11.0, 'mean_rewards': 2.7, 'mean_loss': '0.00101', 'estimating_reward': False, 'reward_exponent': 12.421428571428573}\n",
      "{'time': '0:39:55.844071', 'episode': 820, 'total_frames': 153538.0, 'epsilon': '0.100', 'highest_reward': 11.0, 'mean_rewards': 1.8, 'mean_loss': '0.00091', 'estimating_reward': False, 'reward_exponent': 12.700000000000001}\n",
      "{'time': '0:40:46.954710', 'episode': 830, 'total_frames': 155624.0, 'epsilon': '0.100', 'highest_reward': 11.0, 'mean_rewards': 2.2, 'mean_loss': '0.00050', 'estimating_reward': False, 'reward_exponent': 12.97857142857143}\n",
      "{'time': '0:41:54.954847', 'episode': 840, 'total_frames': 158385.0, 'epsilon': '0.100', 'highest_reward': 11.0, 'mean_rewards': 4.0, 'mean_loss': '0.00046', 'estimating_reward': False, 'reward_exponent': 13.257142857142858}\n",
      "Updating the target network\n",
      "{'time': '0:43:12.672097', 'episode': 850, 'total_frames': 161538.0, 'epsilon': '0.100', 'highest_reward': 14.0, 'mean_rewards': 5.6, 'mean_loss': '0.00044', 'estimating_reward': False, 'reward_exponent': 13.535714285714286}\n",
      "{'time': '0:44:19.817688', 'episode': 860, 'total_frames': 164294.0, 'epsilon': '0.100', 'highest_reward': 14.0, 'mean_rewards': 4.4, 'mean_loss': '0.00106', 'estimating_reward': False, 'reward_exponent': 13.814285714285715}\n",
      "{'time': '0:45:22.859714', 'episode': 870, 'total_frames': 166851.0, 'epsilon': '0.100', 'highest_reward': 14.0, 'mean_rewards': 3.7, 'mean_loss': '0.00075', 'estimating_reward': False, 'reward_exponent': 14.092857142857143}\n",
      "{'time': '0:46:17.824730', 'episode': 880, 'total_frames': 169081.0, 'epsilon': '0.100', 'highest_reward': 14.0, 'mean_rewards': 2.6, 'mean_loss': '0.00063', 'estimating_reward': False, 'reward_exponent': 14.371428571428572}\n",
      "Updating the target network\n",
      "{'time': '0:47:10.086433', 'episode': 890, 'total_frames': 171223.0, 'epsilon': '0.100', 'highest_reward': 14.0, 'mean_rewards': 2.8, 'mean_loss': '0.00160', 'estimating_reward': False, 'reward_exponent': 14.65}\n",
      "{'time': '0:48:15.902103', 'episode': 900, 'total_frames': 173906.0, 'epsilon': '0.100', 'highest_reward': 14.0, 'mean_rewards': 4.2, 'mean_loss': '0.00114', 'estimating_reward': False, 'reward_exponent': 14.928571428571429}\n",
      "{'time': '0:49:23.910635', 'episode': 910, 'total_frames': 176670.0, 'epsilon': '0.100', 'highest_reward': 14.0, 'mean_rewards': 4.1, 'mean_loss': '0.00045', 'estimating_reward': False, 'reward_exponent': 15.207142857142857}\n",
      "{'time': '0:50:21.409406', 'episode': 920, 'total_frames': 179019.0, 'epsilon': '0.100', 'highest_reward': 14.0, 'mean_rewards': 3.6, 'mean_loss': '0.00070', 'estimating_reward': False, 'reward_exponent': 15.485714285714286}\n",
      "Updating the target network\n",
      "{'time': '0:51:29.471487', 'episode': 930, 'total_frames': 181785.0, 'epsilon': '0.100', 'highest_reward': 14.0, 'mean_rewards': 3.9, 'mean_loss': '0.00055', 'estimating_reward': False, 'reward_exponent': 15.764285714285714}\n",
      "{'time': '0:52:29.093063', 'episode': 940, 'total_frames': 184206.0, 'epsilon': '0.100', 'highest_reward': 14.0, 'mean_rewards': 3.6, 'mean_loss': '0.00082', 'estimating_reward': False, 'reward_exponent': 16.042857142857144}\n",
      "{'time': '0:53:36.151754', 'episode': 950, 'total_frames': 186943.0, 'epsilon': '0.100', 'highest_reward': 14.0, 'mean_rewards': 4.6, 'mean_loss': '0.00058', 'estimating_reward': False, 'reward_exponent': 16.32142857142857}\n",
      "Updating the target network\n",
      "{'time': '0:54:53.389403', 'episode': 960, 'total_frames': 190098.0, 'epsilon': '0.100', 'highest_reward': 14.0, 'mean_rewards': 5.9, 'mean_loss': '0.00217', 'estimating_reward': False, 'reward_exponent': 16.6}\n",
      "{'time': '0:56:12.701409', 'episode': 970, 'total_frames': 193318.0, 'epsilon': '0.100', 'highest_reward': 17.0, 'mean_rewards': 7.2, 'mean_loss': '0.00073', 'estimating_reward': False, 'reward_exponent': 16.87857142857143}\n",
      "{'time': '0:57:21.370447', 'episode': 980, 'total_frames': 196122.0, 'epsilon': '0.100', 'highest_reward': 17.0, 'mean_rewards': 4.6, 'mean_loss': '0.00057', 'estimating_reward': False, 'reward_exponent': 17.15714285714286}\n",
      "{'time': '0:58:31.071693', 'episode': 990, 'total_frames': 198956.0, 'epsilon': '0.100', 'highest_reward': 17.0, 'mean_rewards': 5.7, 'mean_loss': '0.00203', 'estimating_reward': False, 'reward_exponent': 17.435714285714287}\n",
      "Updating the target network\n",
      "{'time': '0:59:57.234689', 'episode': 1000, 'total_frames': 202451.0, 'epsilon': '0.100', 'highest_reward': 17.0, 'mean_rewards': 6.7, 'mean_loss': '0.00087', 'estimating_reward': False, 'reward_exponent': 17.714285714285715}\n",
      "{'time': '1:01:16.957511', 'episode': 1010, 'total_frames': 205681.0, 'epsilon': '0.100', 'highest_reward': 17.0, 'mean_rewards': 5.6, 'mean_loss': '0.00047', 'estimating_reward': False, 'reward_exponent': 17.992857142857144}\n",
      "{'time': '1:02:35.060076', 'episode': 1020, 'total_frames': 208877.0, 'epsilon': '0.100', 'highest_reward': 17.0, 'mean_rewards': 7.2, 'mean_loss': '0.00064', 'estimating_reward': False, 'reward_exponent': 18.271428571428572}\n",
      "Updating the target network\n",
      "{'time': '1:04:08.414704', 'episode': 1030, 'total_frames': 212685.0, 'epsilon': '0.100', 'highest_reward': 17.0, 'mean_rewards': 9.4, 'mean_loss': '0.00045', 'estimating_reward': False, 'reward_exponent': 18.55}\n",
      "{'time': '1:05:39.895377', 'episode': 1040, 'total_frames': 216426.0, 'epsilon': '0.100', 'highest_reward': 17.0, 'mean_rewards': 7.0, 'mean_loss': '0.00226', 'estimating_reward': False, 'reward_exponent': 18.82857142857143}\n",
      "{'time': '1:07:01.497968', 'episode': 1050, 'total_frames': 219756.0, 'epsilon': '0.100', 'highest_reward': 17.0, 'mean_rewards': 7.3, 'mean_loss': '0.00116', 'estimating_reward': False, 'reward_exponent': 19.107142857142858}\n",
      "Updating the target network\n",
      "{'time': '1:08:44.100210', 'episode': 1060, 'total_frames': 223943.0, 'epsilon': '0.100', 'highest_reward': 17.0, 'mean_rewards': 9.6, 'mean_loss': '0.00094', 'estimating_reward': False, 'reward_exponent': 19.385714285714286}\n",
      "{'time': '1:10:54.254822', 'episode': 1070, 'total_frames': 229248.0, 'epsilon': '0.100', 'highest_reward': 24.0, 'mean_rewards': 14.1, 'mean_loss': '0.00067', 'estimating_reward': False, 'reward_exponent': 19.664285714285715}\n",
      "Updating the target network\n",
      "{'time': '1:12:53.538947', 'episode': 1080, 'total_frames': 234080.0, 'epsilon': '0.100', 'highest_reward': 24.0, 'mean_rewards': 12.3, 'mean_loss': '0.00069', 'estimating_reward': False, 'reward_exponent': 19.942857142857143}\n",
      "{'time': '1:15:04.870671', 'episode': 1090, 'total_frames': 239414.0, 'epsilon': '0.100', 'highest_reward': 24.0, 'mean_rewards': 15.4, 'mean_loss': '0.00113', 'estimating_reward': False, 'reward_exponent': 20.22142857142857}\n",
      "Updating the target network\n",
      "{'time': '1:17:21.961076', 'episode': 1100, 'total_frames': 244973.0, 'epsilon': '0.100', 'highest_reward': 24.0, 'mean_rewards': 17.4, 'mean_loss': '0.00082', 'estimating_reward': False, 'reward_exponent': 20.5}\n",
      "Updating the target network\n",
      "{'time': '1:19:43.128986', 'episode': 1110, 'total_frames': 250636.0, 'epsilon': '0.100', 'highest_reward': 24.0, 'mean_rewards': 15.4, 'mean_loss': '0.00145', 'estimating_reward': False, 'reward_exponent': 20.77857142857143}\n",
      "{'time': '1:22:14.908612', 'episode': 1120, 'total_frames': 256666.0, 'epsilon': '0.100', 'highest_reward': 24.0, 'mean_rewards': 17.9, 'mean_loss': '0.00387', 'estimating_reward': False, 'reward_exponent': 21.057142857142857}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating the target network\n",
      "{'time': '1:24:37.995018', 'episode': 1130, 'total_frames': 262426.0, 'epsilon': '0.100', 'highest_reward': 36.0, 'mean_rewards': 17.9, 'mean_loss': '0.00123', 'estimating_reward': False, 'reward_exponent': 21.335714285714285}\n",
      "{'time': '1:27:08.256590', 'episode': 1140, 'total_frames': 268540.0, 'epsilon': '0.100', 'highest_reward': 36.0, 'mean_rewards': 18.2, 'mean_loss': '0.00146', 'estimating_reward': False, 'reward_exponent': 21.614285714285714}\n",
      "Updating the target network\n",
      "{'time': '1:29:27.571427', 'episode': 1150, 'total_frames': 274216.0, 'epsilon': '0.100', 'highest_reward': 36.0, 'mean_rewards': 16.8, 'mean_loss': '0.00164', 'estimating_reward': False, 'reward_exponent': 21.892857142857142}\n",
      "Updating the target network\n",
      "{'time': '1:31:58.905322', 'episode': 1160, 'total_frames': 280391.0, 'epsilon': '0.100', 'highest_reward': 36.0, 'mean_rewards': 20.0, 'mean_loss': '0.00143', 'estimating_reward': False, 'reward_exponent': 22.17142857142857}\n",
      "{'time': '1:34:30.820959', 'episode': 1170, 'total_frames': 286403.0, 'epsilon': '0.100', 'highest_reward': 36.0, 'mean_rewards': 18.2, 'mean_loss': '0.00121', 'estimating_reward': False, 'reward_exponent': 22.45}\n",
      "Updating the target network\n",
      "{'time': '1:37:04.096621', 'episode': 1180, 'total_frames': 292660.0, 'epsilon': '0.100', 'highest_reward': 36.0, 'mean_rewards': 17.9, 'mean_loss': '0.00162', 'estimating_reward': False, 'reward_exponent': 22.728571428571428}\n",
      "{'time': '1:39:31.959838', 'episode': 1190, 'total_frames': 298685.0, 'epsilon': '0.100', 'highest_reward': 36.0, 'mean_rewards': 18.8, 'mean_loss': '0.00212', 'estimating_reward': False, 'reward_exponent': 23.007142857142856}\n",
      "Updating the target network\n",
      "{'time': '1:42:11.694894', 'episode': 1200, 'total_frames': 305167.0, 'epsilon': '0.100', 'highest_reward': 36.0, 'mean_rewards': 20.2, 'mean_loss': '0.00152', 'estimating_reward': False, 'reward_exponent': 23.285714285714285}\n",
      "Updating the target network\n",
      "{'time': '1:44:42.713129', 'episode': 1210, 'total_frames': 311285.0, 'epsilon': '0.100', 'highest_reward': 36.0, 'mean_rewards': 19.3, 'mean_loss': '0.00211', 'estimating_reward': False, 'reward_exponent': 23.564285714285717}\n",
      "{'time': '1:47:21.783768', 'episode': 1220, 'total_frames': 317637.0, 'epsilon': '0.100', 'highest_reward': 40.0, 'mean_rewards': 17.9, 'mean_loss': '0.00116', 'estimating_reward': False, 'reward_exponent': 23.842857142857145}\n",
      "Updating the target network\n",
      "{'time': '1:50:07.224277', 'episode': 1230, 'total_frames': 324415.0, 'epsilon': '0.100', 'highest_reward': 40.0, 'mean_rewards': 19.8, 'mean_loss': '0.00192', 'estimating_reward': False, 'reward_exponent': 24.121428571428574}\n",
      "Updating the target network\n",
      "{'time': '1:52:52.905014', 'episode': 1240, 'total_frames': 331134.0, 'epsilon': '0.100', 'highest_reward': 41.0, 'mean_rewards': 23.2, 'mean_loss': '0.00195', 'estimating_reward': False, 'reward_exponent': 24.400000000000002}\n",
      "{'time': '1:55:55.282369', 'episode': 1250, 'total_frames': 338594.0, 'epsilon': '0.100', 'highest_reward': 43.0, 'mean_rewards': 24.8, 'mean_loss': '0.00125', 'estimating_reward': False, 'reward_exponent': 24.67857142857143}\n",
      "Updating the target network\n",
      "{'time': '1:58:32.813471', 'episode': 1260, 'total_frames': 344996.0, 'epsilon': '0.100', 'highest_reward': 43.0, 'mean_rewards': 18.1, 'mean_loss': '0.00173', 'estimating_reward': False, 'reward_exponent': 24.95714285714286}\n",
      "Updating the target network\n",
      "{'time': '2:01:27.917908', 'episode': 1270, 'total_frames': 352169.0, 'epsilon': '0.100', 'highest_reward': 43.0, 'mean_rewards': 22.5, 'mean_loss': '0.00244', 'estimating_reward': False, 'reward_exponent': 25.235714285714288}\n",
      "Updating the target network\n",
      "{'time': '2:04:53.989541', 'episode': 1280, 'total_frames': 360632.0, 'epsilon': '0.100', 'highest_reward': 43.0, 'mean_rewards': 27.0, 'mean_loss': '0.00163', 'estimating_reward': False, 'reward_exponent': 25.514285714285716}\n",
      "{'time': '2:08:04.629834', 'episode': 1290, 'total_frames': 368411.0, 'epsilon': '0.100', 'highest_reward': 52.0, 'mean_rewards': 27.0, 'mean_loss': '0.00200', 'estimating_reward': False, 'reward_exponent': 25.792857142857144}\n",
      "Updating the target network\n",
      "{'time': '2:11:14.538112', 'episode': 1300, 'total_frames': 376086.0, 'epsilon': '0.100', 'highest_reward': 52.0, 'mean_rewards': 24.9, 'mean_loss': '0.00281', 'estimating_reward': False, 'reward_exponent': 26.071428571428573}\n",
      "Updating the target network\n",
      "{'time': '2:14:51.258579', 'episode': 1310, 'total_frames': 384902.0, 'epsilon': '0.100', 'highest_reward': 52.0, 'mean_rewards': 30.7, 'mean_loss': '0.00739', 'estimating_reward': False, 'reward_exponent': 26.35}\n",
      "Updating the target network\n",
      "{'time': '2:18:15.084702', 'episode': 1320, 'total_frames': 393172.0, 'epsilon': '0.100', 'highest_reward': 52.0, 'mean_rewards': 28.2, 'mean_loss': '0.00227', 'estimating_reward': False, 'reward_exponent': 26.62857142857143}\n",
      "Updating the target network\n",
      "{'time': '2:21:20.167939', 'episode': 1330, 'total_frames': 400695.0, 'epsilon': '0.100', 'highest_reward': 52.0, 'mean_rewards': 24.7, 'mean_loss': '0.00483', 'estimating_reward': False, 'reward_exponent': 26.90714285714286}\n",
      "{'time': '2:24:01.017754', 'episode': 1340, 'total_frames': 407257.0, 'epsilon': '0.100', 'highest_reward': 52.0, 'mean_rewards': 21.4, 'mean_loss': '0.00158', 'estimating_reward': False, 'reward_exponent': 27.185714285714287}\n",
      "Updating the target network\n",
      "{'time': '2:27:19.252412', 'episode': 1350, 'total_frames': 415324.0, 'epsilon': '0.100', 'highest_reward': 52.0, 'mean_rewards': 26.5, 'mean_loss': '0.00163', 'estimating_reward': False, 'reward_exponent': 27.464285714285715}\n",
      "Updating the target network\n",
      "{'time': '2:30:34.862784', 'episode': 1360, 'total_frames': 423351.0, 'epsilon': '0.100', 'highest_reward': 52.0, 'mean_rewards': 27.4, 'mean_loss': '0.00287', 'estimating_reward': False, 'reward_exponent': 27.742857142857144}\n",
      "Updating the target network\n",
      "{'time': '2:34:04.223207', 'episode': 1370, 'total_frames': 431906.0, 'epsilon': '0.100', 'highest_reward': 52.0, 'mean_rewards': 29.2, 'mean_loss': '0.00468', 'estimating_reward': False, 'reward_exponent': 28.021428571428572}\n",
      "Updating the target network\n",
      "{'time': '2:38:09.253600', 'episode': 1380, 'total_frames': 441904.0, 'epsilon': '0.100', 'highest_reward': 72.0, 'mean_rewards': 35.2, 'mean_loss': '0.00231', 'estimating_reward': False, 'reward_exponent': 28.3}\n",
      "Updating the target network\n",
      "{'time': '2:42:02.350663', 'episode': 1390, 'total_frames': 451389.0, 'epsilon': '0.100', 'highest_reward': 72.0, 'mean_rewards': 31.8, 'mean_loss': '0.00247', 'estimating_reward': False, 'reward_exponent': 28.57857142857143}\n",
      "Updating the target network\n",
      "{'time': '2:46:09.854886', 'episode': 1400, 'total_frames': 461441.0, 'epsilon': '0.100', 'highest_reward': 72.0, 'mean_rewards': 33.8, 'mean_loss': '0.00196', 'estimating_reward': False, 'reward_exponent': 28.857142857142858}\n",
      "{'time': '2:49:27.882096', 'episode': 1410, 'total_frames': 469530.0, 'epsilon': '0.100', 'highest_reward': 72.0, 'mean_rewards': 27.6, 'mean_loss': '0.00372', 'estimating_reward': False, 'reward_exponent': 29.135714285714286}\n",
      "Updating the target network\n",
      "{'time': '2:52:45.428772', 'episode': 1420, 'total_frames': 477577.0, 'epsilon': '0.100', 'highest_reward': 72.0, 'mean_rewards': 27.5, 'mean_loss': '0.00233', 'estimating_reward': False, 'reward_exponent': 29.414285714285715}\n",
      "Updating the target network\n",
      "{'time': '2:56:15.160202', 'episode': 1430, 'total_frames': 485961.0, 'epsilon': '0.100', 'highest_reward': 72.0, 'mean_rewards': 29.2, 'mean_loss': '0.00337', 'estimating_reward': False, 'reward_exponent': 29.692857142857143}\n",
      "Updating the target network\n",
      "{'time': '2:59:44.704285', 'episode': 1440, 'total_frames': 494485.0, 'epsilon': '0.100', 'highest_reward': 72.0, 'mean_rewards': 29.6, 'mean_loss': '0.00443', 'estimating_reward': False, 'reward_exponent': 29.97142857142857}\n",
      "Updating the target network\n",
      "{'time': '3:03:18.867270', 'episode': 1450, 'total_frames': 503239.0, 'epsilon': '0.100', 'highest_reward': 72.0, 'mean_rewards': 30.0, 'mean_loss': '0.00366', 'estimating_reward': False, 'reward_exponent': 30.25}\n",
      "Updating the target network\n",
      "{'time': '3:07:21.692245', 'episode': 1460, 'total_frames': 513133.0, 'epsilon': '0.100', 'highest_reward': 72.0, 'mean_rewards': 37.4, 'mean_loss': '0.00320', 'estimating_reward': False, 'reward_exponent': 30.52857142857143}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating the target network\n",
      "{'time': '3:11:16.063723', 'episode': 1470, 'total_frames': 522632.0, 'epsilon': '0.100', 'highest_reward': 72.0, 'mean_rewards': 32.1, 'mean_loss': '0.00203', 'estimating_reward': False, 'reward_exponent': 30.807142857142857}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-80264a42a3bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mGAME_ENV\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgames\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0mhandler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHandleResults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGAME_ENV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOUT_FOLDER\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     \u001b[0mlearn_by_game\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-80264a42a3bb>\u001b[0m in \u001b[0;36mlearn_by_game\u001b[0;34m(results_handler, load_folder, load_model)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepisode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_number_of_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0mepisode_reward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_frames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_episode_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgame_env\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_frames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;31m# all_rewards[episode] = episode_reward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-80264a42a3bb>\u001b[0m in \u001b[0;36mrun_episode\u001b[0;34m(max_episode_length, episode, game_env, player, total_frames, evaluation)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mevaluation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0mplayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_frames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessed_new_frame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminal_life_lost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlife_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mepisode_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/atari_agent/player/player.py\u001b[0m in \u001b[0;36mupdates\u001b[0;34m(self, no_passed_frames, episode, action, processed_new_frame, reward, terminal_life_lost, episode_seq)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mno_passed_frames\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexploratory_memory_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_epsilon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mno_passed_frames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;31m# @jit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/atari_agent/player/player.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, no_passed_frames)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mno_passed_frames\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_frequency\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mcurrent_state_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminal_flags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_minibatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_state_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminal_flags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/atari_agent/player/player_components/learner.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, current_state_batch, actions, rewards, next_state_batch, terminal_flags)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         history = self.main_learner.model.fit([current_state_batch, one_hot_actions], one_hot_targets,\n\u001b[0;32m---> 79\u001b[0;31m                                  epochs=1, batch_size=self.batch_size, verbose=0, callbacks=self.tbCallBack)\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1449\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[0;32m-> 1451\u001b[0;31m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[1;32m   1452\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from player.player import Player\n",
    "from environments.simulator import Atari\n",
    "import numpy as np\n",
    "import datetime\n",
    "from utils import HandleResults\n",
    "import numba\n",
    "\n",
    "'''\n",
    "Set the main settings in the default_settings.jsn\n",
    "* PUNISH controls the positive punishment, to be set to non-zero to work\n",
    "* REWARD_EXTRAPOLATION_EXPONENT controls the exponent for backfilling. Set to -1.0 to turn this off (i.e., use the \n",
    "actual reward values only)\n",
    "\n",
    "Some settings are in the memory class:\n",
    "* START_EPISODE: This will be the start episode of the linear increase. \n",
    "* END_EPISODE: This will be the end episode of the linear increase.\n",
    "* START_EXPONENT: The start exponent (1.0)\n",
    "* END_EXPONENT: Final exponent value (10.0)\n",
    "* IGNORE_EXPONENT_EPISODE: At what episode ignore using the exponent (just punishment if on)\n",
    " '''\n",
    "\n",
    "\n",
    "def run_episode(max_episode_length, episode, game_env, player, total_frames, evaluation=False):\n",
    "    terminal_life_lost = game_env.reset()\n",
    "    episode_reward = 0\n",
    "    life_seq = 0\n",
    "    frame_number = 0\n",
    "    gif_frames = []\n",
    "    while True:\n",
    "        # Get state, make action, get next state (rewards, terminal, ...), record the experience, train if necessary\n",
    "        current_state = game_env.get_current_state()\n",
    "        action = player.take_action(current_state, total_frames, evaluation)\n",
    "        processed_new_frame, reward, terminal, terminal_life_lost, original_frame = game_env.step(action)\n",
    "\n",
    "        if frame_number >= max_episode_length:\n",
    "            terminal = True\n",
    "            terminal_life_lost = True\n",
    "\n",
    "        # if evaluation:\n",
    "        #     gif_frames.append(original_frame)\n",
    "\n",
    "        if not evaluation:\n",
    "            player.updates(total_frames, episode, action, processed_new_frame, reward, terminal_life_lost, life_seq)\n",
    "\n",
    "        episode_reward += reward\n",
    "        life_seq += 1\n",
    "\n",
    "        if terminal_life_lost:\n",
    "            life_seq = 0\n",
    "\n",
    "        # game_env.env.render()\n",
    "        total_frames += 1\n",
    "        frame_number += 1\n",
    "\n",
    "        if terminal:\n",
    "            break\n",
    "\n",
    "    return episode_reward, total_frames\n",
    "\n",
    "\n",
    "def learn_by_game(results_handler, load_folder='', load_model=False):\n",
    "\n",
    "    if load_folder is not '':\n",
    "        player, game_env, max_episode_length, max_number_of_episodes, all_settings = \\\n",
    "            results_handler.load_settings_folder(load_folder, load_model)\n",
    "    else:\n",
    "        player, game_env, max_episode_length, max_number_of_episodes, all_settings = \\\n",
    "            results_handler.load_settings_default(GAME_ENV)\n",
    "\n",
    "    for k, v in all_settings.items():\n",
    "        print(k, ': ', v)\n",
    "\n",
    "    print('****************************')\n",
    "\n",
    "    results_handler.save_settings(all_settings, player)\n",
    "    res_dict = {}\n",
    "\n",
    "    highest_reward = 0\n",
    "    total_frames = 0.0\n",
    "    prev_frames = 0.0\n",
    "    all_rewards = []\n",
    "    time = datetime.datetime.now()\n",
    "    prev_time = time\n",
    "    best_evaluation = 0\n",
    "\n",
    "    for episode in range(max_number_of_episodes):\n",
    "        episode_reward, total_frames = run_episode(max_episode_length, episode, game_env, player, total_frames)\n",
    "\n",
    "        # all_rewards[episode] = episode_reward\n",
    "        all_rewards.append(episode_reward)\n",
    "\n",
    "        if episode_reward>highest_reward:\n",
    "            highest_reward = episode_reward\n",
    "\n",
    "        if episode % 10 == 0:\n",
    "            # evaluation_reward, _ = run_episode(max_episode_length, episode, game_env, player, 0, evaluation=True)\n",
    "\n",
    "            # if evaluation_reward > best_evaluation:\n",
    "            #     best_evaluation = evaluation_reward\n",
    "                # print('Best eval: ', str(best_evaluation))\n",
    "\n",
    "            now = datetime.datetime.now()\n",
    "            res_dict['time'] = str(now - time)\n",
    "            res_dict['episode'] = episode\n",
    "            res_dict['total_frames'] = total_frames\n",
    "            res_dict['epsilon'] = format(player.epsilon, '.3f')\n",
    "            res_dict['highest_reward'] = highest_reward\n",
    "            # res_dict['best_eval'] = best_evaluation\n",
    "            res_dict['mean_rewards'] = np.mean(all_rewards[-10:])\n",
    "            res_dict['mean_loss'] = format(np.mean(player.losses[-10:]), '.5f')\n",
    "            # res_dict['memory_vol'] = player.memory.count\n",
    "            # res_dict['fps'] = (total_frames - prev_frames) / ((now - prev_time).total_seconds())\n",
    "            # res_dict['sparsity'] = np.mean(player.memory.sparsity_lengths[-10:])\n",
    "            res_dict['estimating_reward'] = player.memory.use_estimated_reward\n",
    "            res_dict['reward_exponent'] = player.memory.reward_extrapolation_exponent\n",
    "\n",
    "            results_handler.save_res(res_dict)\n",
    "\n",
    "            prev_time = now\n",
    "            prev_frames = total_frames\n",
    "\n",
    "# GAME_ENV = 'BreakoutDeterministic-v4'\n",
    "# GAME_ENV = 'BerzerkDeterministic-v4'\n",
    "# GAME_ENV = 'QbertDeterministic-v4'\n",
    "# GAME_ENV = 'SpaceInvaders-v4' # 758 frames\n",
    "# GAME_ENV = 'Alien-v4' # 948 frames\n",
    "# GAME_ENV = 'Amidar-v4' # 812 frames\n",
    "# GAME_ENV = 'Venture-v4'\n",
    "# GAME_ENV = 'Assault-v4' # 876 frames\n",
    "# GAME_ENV = 'RoadRunner-v4' # 437 frames\n",
    "# GAME_ENV = 'PongDeterministic-v4'\n",
    "# GAME_ENV = 'AsterixDeterministic-v4'\n",
    "# GAME_ENV = 'MontezumaRevenge-v4'\n",
    "# GAME_ENV = 'ChopperCommand-v4'\n",
    "# OUT_FOLDER = './output/Punish_0_No_Reward_exploration/'\n",
    "# OUT_FOLDER = './output/Punish_1_No_Reward_exploration/'\n",
    "# OUT_FOLDER = './output/Punish_1_Reward_exploration_linear/'\n",
    "OUT_FOLDER = './output/punish/'\n",
    "\n",
    "# games = [, 'QbertDeterministic-v4']\n",
    "games = ['BreakoutDeterministic-v4', 'AsterixDeterministic-v4', 'CarnivalDeterministic-v4', 'MsPacmanDeterministic-v4', \n",
    "         'UpNDownDeterministic-v4', 'AssaultDeterministic-v4']\n",
    "\n",
    "\n",
    "for GAME_ENV in games:\n",
    "    handler = HandleResults(GAME_ENV, OUT_FOLDER)\n",
    "    learn_by_game(handler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
