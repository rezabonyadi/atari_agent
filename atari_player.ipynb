{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from player.player import Player\n",
    "from environments.simulator import Atari\n",
    "import numpy as np\n",
    "import datetime\n",
    "from utils import HandleResults\n",
    "import numba\n",
    "\n",
    "GAME_ENV = 'BreakoutDeterministic-v4'\n",
    "# GAME_ENV = 'SpaceInvaders-v4' # 758 frames\n",
    "# GAME_ENV = 'Alien-v4' # 948 frames\n",
    "# GAME_ENV = 'Amidar-v4' # 812 frames\n",
    "# GAME_ENV = 'Venture-v4'\n",
    "# GAME_ENV = 'Assault-v4' # 876 frames\n",
    "# GAME_ENV = 'RoadRunner-v4' # 437 frames\n",
    "# GAME_ENV = 'PongDeterministic-v4'\n",
    "# GAME_ENV = 'Asterix-v4'\n",
    "# GAME_ENV = 'MontezumaRevenge-v4'\n",
    "# GAME_ENV = 'ChopperCommand-v4'\n",
    "# OUT_FOLDER = './output/Punish_0_No_Reward_exploration/'\n",
    "# OUT_FOLDER = './output/Punish_1_No_Reward_exploration/'\n",
    "OUT_FOLDER = './output/Punish_1_Reward_exploration_2/'\n",
    "\n",
    "results_handler = HandleResults(GAME_ENV, OUT_FOLDER)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(max_episode_length, episode, game_env, player, total_frames, evaluation=False):\n",
    "    terminal_life_lost = game_env.reset()\n",
    "    episode_reward = 0\n",
    "    life_seq = 0\n",
    "    frame_number = 0\n",
    "    gif_frames = []\n",
    "    while True:\n",
    "        # Get state, make action, get next state (rewards, terminal, ...), record the experience, train if necessary\n",
    "        current_state = game_env.get_current_state()\n",
    "        action = player.take_action(current_state, episode, evaluation)\n",
    "        processed_new_frame, reward, terminal, terminal_life_lost, original_frame = game_env.step(action)\n",
    "\n",
    "        if frame_number >= max_episode_length:\n",
    "            terminal = True\n",
    "            terminal_life_lost = True\n",
    "\n",
    "        # if evaluation:\n",
    "        #     gif_frames.append(original_frame)\n",
    "\n",
    "        if not evaluation:\n",
    "            player.updates(total_frames, episode, action, processed_new_frame, reward, terminal_life_lost, life_seq)\n",
    "\n",
    "        episode_reward += reward\n",
    "        life_seq += 1\n",
    "\n",
    "        if terminal_life_lost:\n",
    "            life_seq = 0\n",
    "\n",
    "        # game_env.env.render()\n",
    "        total_frames += 1\n",
    "        frame_number += 1\n",
    "\n",
    "        if terminal:\n",
    "            break\n",
    "\n",
    "    return episode_reward, total_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The environment has the following 4 actions: ['NOOP', 'FIRE', 'RIGHT', 'LEFT']\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 84, 84, 4)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "norm (Lambda)                   (None, 84, 84, 4)    0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 20, 20, 32)   8224        norm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 9, 9, 64)     32832       conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 7, 7, 64)     36928       conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 3136)         0           conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 512)          1606144     flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 4)            2052        dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "action_mask (InputLayer)        (None, 4)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "QValue (Multiply)               (None, 4)            0           dense_4[0][0]                    \n",
      "                                                                 action_mask[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 1,686,180\n",
      "Trainable params: 1,686,180\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\losses\\losses_impl.py:448: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 84, 84, 4)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "norm (Lambda)                   (None, 84, 84, 4)    0           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 20, 20, 32)   8224        norm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 9, 9, 64)     32832       conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 7, 7, 64)     36928       conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 3136)         0           conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 512)          1606144     flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 4)            2052        dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "action_mask (InputLayer)        (None, 4)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "QValue (Multiply)               (None, 4)            0           dense_6[0][0]                    \n",
      "                                                                 action_mask[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 1,686,180\n",
      "Trainable params: 1,686,180\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "load_folder=''\n",
    "load_model=False\n",
    "\n",
    "if load_folder is not '':\n",
    "    player, game_env, max_episode_length, max_number_of_episodes, all_settings = \\\n",
    "        results_handler.load_settings(load_folder, load_model)\n",
    "else:\n",
    "    player, game_env, max_episode_length, max_number_of_episodes, all_settings = \\\n",
    "        results_handler.load_default_settings_constants(GAME_ENV)\n",
    "\n",
    "results_handler.save_settings(all_settings, player)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\skimage\\transform\\_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'time': '0:00:01.563906', 'episode': 0, 'total_frames': 199.0, 'epsilon': '1.000', 'highest_reward': 1.0, 'mean_rewards': 1.0, 'mean_loss': 'nan', 'sparsity': 31.333333333333332}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "res_dict = {}\n",
    "\n",
    "highest_reward = 0\n",
    "total_frames = 0.0\n",
    "prev_frames = 0.0\n",
    "all_rewards = []\n",
    "time = datetime.datetime.now()\n",
    "prev_time = time\n",
    "best_evaluation = 0\n",
    "max_number_of_episodes = 1\n",
    "\n",
    "for episode in range(max_number_of_episodes):\n",
    "    episode_reward, total_frames = run_episode(max_episode_length, episode, game_env, player, total_frames)\n",
    "\n",
    "    # all_rewards[episode] = episode_reward\n",
    "    all_rewards.append(episode_reward)\n",
    "\n",
    "    if episode_reward>highest_reward:\n",
    "        highest_reward = episode_reward\n",
    "\n",
    "    if episode % 10 == 0:\n",
    "        # evaluation_reward, _ = run_episode(max_episode_length, episode, game_env, player, 0, evaluation=True)\n",
    "\n",
    "        # if evaluation_reward > best_evaluation:\n",
    "        #     best_evaluation = evaluation_reward\n",
    "            # print('Best eval: ', str(best_evaluation))\n",
    "\n",
    "        now = datetime.datetime.now()\n",
    "        res_dict['time'] = str(now - time)\n",
    "        res_dict['episode'] = episode\n",
    "        res_dict['total_frames'] = total_frames\n",
    "        res_dict['epsilon'] = format(player.epsilon, '.3f')\n",
    "        res_dict['highest_reward'] = highest_reward\n",
    "        # res_dict['best_eval'] = best_evaluation\n",
    "        res_dict['mean_rewards'] = np.mean(all_rewards[-10:])\n",
    "        res_dict['mean_loss'] = format(np.mean(player.losses[-10:]), '.5f')\n",
    "        # res_dict['memory_vol'] = player.memory.count\n",
    "        # res_dict['fps'] = (total_frames - prev_frames) / ((now - prev_time).total_seconds())\n",
    "        res_dict['sparsity'] = np.mean(player.memory.sparsity_lengths[-10:])\n",
    "\n",
    "        results_handler.save_res(res_dict)\n",
    "\n",
    "        prev_time = now\n",
    "        prev_frames = total_frames\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
