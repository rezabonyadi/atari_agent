{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from player.player import Player\n",
    "from environments.simulator import Atari\n",
    "import numpy as np\n",
    "import datetime\n",
    "from utils import HandleResults\n",
    "import numba\n",
    "\n",
    "\n",
    "GAME_ENV = 'BreakoutDeterministic-v4'\n",
    "# GAME_ENV = 'SpaceInvaders-v4' # 758 frames\n",
    "# GAME_ENV = 'Alien-v4' # 948 frames\n",
    "# GAME_ENV = 'Amidar-v4' # 812 frames\n",
    "# GAME_ENV = 'Venture-v4'\n",
    "# GAME_ENV = 'Assault-v4' # 876 frames\n",
    "# GAME_ENV = 'RoadRunner-v4' # 437 frames\n",
    "# GAME_ENV = 'PongDeterministic-v4'\n",
    "# GAME_ENV = 'Asterix-v4'\n",
    "# GAME_ENV = 'MontezumaRevenge-v4'\n",
    "# GAME_ENV = 'ChopperCommand-v4'\n",
    "# OUT_FOLDER = './output/Punish_0_No_Reward_exploration/'\n",
    "# OUT_FOLDER = './output/Punish_1_No_Reward_exploration/'\n",
    "OUT_FOLDER = './output/Punish_1_Reward_exploration_2/'\n",
    "\n",
    "results_handler = HandleResults(GAME_ENV, OUT_FOLDER)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(max_episode_length, episode, game_env, player, total_frames, evaluation=False):\n",
    "    terminal_life_lost = game_env.reset()\n",
    "    episode_reward = 0\n",
    "    life_seq = 0\n",
    "    frame_number = 0\n",
    "    gif_frames = []\n",
    "    while True:\n",
    "        # Get state, make action, get next state (rewards, terminal, ...), record the experience, train if necessary\n",
    "        current_state = game_env.get_current_state()\n",
    "        action = player.take_action(current_state, total_frames, evaluation)\n",
    "        processed_new_frame, reward, terminal, terminal_life_lost, original_frame = game_env.step(action)\n",
    "\n",
    "        if frame_number >= max_episode_length:\n",
    "            terminal = True\n",
    "            terminal_life_lost = True\n",
    "\n",
    "        # if evaluation:\n",
    "        #     gif_frames.append(original_frame)\n",
    "\n",
    "        if not evaluation:\n",
    "            player.updates(total_frames, episode, action, processed_new_frame, reward, terminal_life_lost, life_seq)\n",
    "\n",
    "        episode_reward += reward\n",
    "        life_seq += 1\n",
    "\n",
    "        if terminal_life_lost:\n",
    "            life_seq = 0\n",
    "\n",
    "        # game_env.env.render()\n",
    "        total_frames += 1\n",
    "        frame_number += 1\n",
    "\n",
    "        if terminal:\n",
    "            break\n",
    "\n",
    "    return episode_reward, total_frames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The environment has the following 4 actions: ['NOOP', 'FIRE', 'RIGHT', 'LEFT']\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 84, 84, 4)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "norm (Lambda)                   (None, 84, 84, 4)    0           input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 20, 20, 32)   8224        norm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 9, 9, 64)     32832       conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 7, 7, 64)     36928       conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 3136)         0           conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 512)          1606144     flatten_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 4)            2052        dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "action_mask (InputLayer)        (None, 4)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "QValue (Multiply)               (None, 4)            0           dense_10[0][0]                   \n",
      "                                                                 action_mask[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 1,686,180\n",
      "Trainable params: 1,686,180\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            (None, 84, 84, 4)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "norm (Lambda)                   (None, 84, 84, 4)    0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 20, 20, 32)   8224        norm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 9, 9, 64)     32832       conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 7, 7, 64)     36928       conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)             (None, 3136)         0           conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 512)          1606144     flatten_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 4)            2052        dense_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "action_mask (InputLayer)        (None, 4)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "QValue (Multiply)               (None, 4)            0           dense_12[0][0]                   \n",
      "                                                                 action_mask[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 1,686,180\n",
      "Trainable params: 1,686,180\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "MAX_EPISODE_LENGTH :  18000\n",
      "NO_OP_STEPS :  10\n",
      "MAX_EPISODES :  2000\n",
      "AGENT_HISTORY_LENGTH :  4\n",
      "UPDATE_FREQ :  4\n",
      "NETW_UPDATE_FREQ :  10000\n",
      "REPLAY_MEMORY_START_SIZE :  50000\n",
      "DISCOUNT_FACTOR :  0.99\n",
      "MEMORY_SIZE :  1000000\n",
      "BS :  32\n",
      "LEARNING_RATE :  0.0001\n",
      "PUNISH :  0.5\n",
      "INI_EPSILON :  1.0\n",
      "END_EPSILON :  0.1\n",
      "MIN_OBSERVE_EPISODE :  200\n",
      "GAME_ENV :  BreakoutDeterministic-v4\n",
      "REWARD_EXTRAPOLATION_EXPONENT :  10.0\n",
      "LINEAR_EXPLORATION_EXPONENT :  True\n",
      "USE_DOUBLE_MODEL :  True\n",
      "frame_height :  84\n",
      "frame_width :  84\n",
      "****************************\n"
     ]
    }
   ],
   "source": [
    "load_folder=''\n",
    "load_model=False\n",
    "\n",
    "if load_folder is not '':\n",
    "    player, game_env, max_episode_length, max_number_of_episodes, all_settings = \\\n",
    "        results_handler.load_settings(load_folder, load_model)\n",
    "else:\n",
    "    player, game_env, max_episode_length, max_number_of_episodes, all_settings = \\\n",
    "        results_handler.load_default_settings(GAME_ENV)\n",
    "\n",
    "for k, v in all_settings.items():\n",
    "    print(k, ': ', v)\n",
    "\n",
    "print('****************************')\n",
    "\n",
    "results_handler.save_settings(all_settings, player)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\skimage\\transform\\_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'time': '0:00:01.563906', 'episode': 0, 'total_frames': 199.0, 'epsilon': '1.000', 'highest_reward': 1.0, 'mean_rewards': 1.0, 'mean_loss': 'nan', 'sparsity': 31.333333333333332}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "res_dict = {}\n",
    "\n",
    "highest_reward = 0\n",
    "total_frames = 0.0\n",
    "prev_frames = 0.0\n",
    "all_rewards = []\n",
    "time = datetime.datetime.now()\n",
    "prev_time = time\n",
    "best_evaluation = 0\n",
    "\n",
    "for episode in range(max_number_of_episodes):\n",
    "    episode_reward, total_frames = run_episode(max_episode_length, episode, game_env, player, total_frames)\n",
    "\n",
    "    # all_rewards[episode] = episode_reward\n",
    "    all_rewards.append(episode_reward)\n",
    "\n",
    "    if episode_reward>highest_reward:\n",
    "        highest_reward = episode_reward\n",
    "\n",
    "    if episode % 10 == 0:\n",
    "        # evaluation_reward, _ = run_episode(max_episode_length, episode, game_env, player, 0, evaluation=True)\n",
    "\n",
    "        # if evaluation_reward > best_evaluation:\n",
    "        #     best_evaluation = evaluation_reward\n",
    "            # print('Best eval: ', str(best_evaluation))\n",
    "\n",
    "        now = datetime.datetime.now()\n",
    "        res_dict['time'] = str(now - time)\n",
    "        res_dict['episode'] = episode\n",
    "        res_dict['total_frames'] = total_frames\n",
    "        res_dict['epsilon'] = format(player.epsilon, '.3f')\n",
    "        res_dict['highest_reward'] = highest_reward\n",
    "        # res_dict['best_eval'] = best_evaluation\n",
    "        res_dict['mean_rewards'] = np.mean(all_rewards[-10:])\n",
    "        res_dict['mean_loss'] = format(np.mean(player.losses[-10:]), '.5f')\n",
    "        # res_dict['memory_vol'] = player.memory.count\n",
    "        # res_dict['fps'] = (total_frames - prev_frames) / ((now - prev_time).total_seconds())\n",
    "        # res_dict['sparsity'] = np.mean(player.memory.sparsity_lengths[-10:])\n",
    "        res_dict['estimating_reward'] = player.memory.use_estimated_reward\n",
    "        res_dict['reward_exponent'] = player.memory.reward_extrapolation_exponent\n",
    "\n",
    "        results_handler.save_res(res_dict)\n",
    "\n",
    "        prev_time = now\n",
    "        prev_frames = total_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdventureDeterministic-v4 ,  Discrete(18) ,  5095.7 ,  -0.2 ,  0.0\n",
      "AirRaidDeterministic-v4 ,  Discrete(6) ,  580.0 ,  595.0 ,  100.0\n",
      "AlienDeterministic-v4 ,  Discrete(18) ,  723.5 ,  221.0 ,  20.0\n",
      "AmidarDeterministic-v4 ,  Discrete(10) ,  612.1 ,  2.9 ,  5.0\n",
      "AssaultDeterministic-v4 ,  Discrete(7) ,  538.1 ,  199.5 ,  21.0\n",
      "AsterixDeterministic-v4 ,  Discrete(9) ,  273.1 ,  300.0 ,  100.0\n",
      "AsteroidsDeterministic-v4 ,  Discrete(14) ,  1086.1 ,  884.0 ,  150.0\n",
      "AtlantisDeterministic-v4 ,  Discrete(4) ,  1286.8 ,  17510.0 ,  3500.0\n",
      "BankHeistDeterministic-v4 ,  Discrete(18) ,  538.6 ,  17.0 ,  20.0\n",
      "BattleZoneDeterministic-v4 ,  Discrete(18) ,  1173.5 ,  2300.0 ,  6000.0\n",
      "BeamRiderDeterministic-v4 ,  Discrete(9) ,  1507.7 ,  369.6 ,  44.0\n",
      "BerzerkDeterministic-v4 ,  Discrete(18) ,  220.3 ,  150.0 ,  50.0\n",
      "BowlingDeterministic-v4 ,  Discrete(6) ,  2104.9 ,  22.7 ,  7.0\n",
      "BoxingDeterministic-v4 ,  Discrete(18) ,  1781.0 ,  3.4 ,  2.0\n",
      "BreakoutDeterministic-v4 ,  Discrete(4) ,  194.6 ,  1.7 ,  1.0\n",
      "CarnivalDeterministic-v4 ,  Discrete(6) ,  372.1 ,  676.0 ,  500.0\n",
      "CentipedeDeterministic-v4 ,  Discrete(18) ,  620.7 ,  2240.6 ,  1000.0\n",
      "ChopperCommandDeterministic-v4 ,  Discrete(18) ,  1004.8 ,  780.0 ,  200.0\n",
      "CrazyClimberDeterministic-v4 ,  Discrete(9) ,  3039.6 ,  7670.0 ,  100.0\n",
      "DefenderDeterministic-v4 ,  Discrete(18) ,  2161.2 ,  474500.0 ,  40005.0\n",
      "DemonAttackDeterministic-v4 ,  Discrete(6) ,  1408.8 ,  211.0 ,  20.0\n",
      "DoubleDunkDeterministic-v4 ,  Discrete(18) ,  2133.7 ,  -19.2 ,  1.0\n",
      "ElevatorActionDeterministic-v4 ,  Discrete(18) ,  6001.0 ,  1040.0 ,  100.0\n",
      "EnduroDeterministic-v4 ,  Discrete(9) ,  3323.0 ,  0.0 ,  0.0\n",
      "FishingDerbyDeterministic-v4 ,  Discrete(18) ,  1851.8 ,  -94.2 ,  1.0\n",
      "FreewayDeterministic-v4 ,  Discrete(3) ,  2043.0 ,  0.0 ,  0.0\n",
      "FrostbiteDeterministic-v4 ,  Discrete(18) ,  377.4 ,  51.0 ,  10.0\n",
      "GopherDeterministic-v4 ,  Discrete(8) ,  1087.6 ,  370.0 ,  120.0\n",
      "GravitarDeterministic-v4 ,  Discrete(18) ,  630.9 ,  225.0 ,  250.0\n",
      "HeroDeterministic-v4 ,  Discrete(18) ,  394.4 ,  785.0 ,  1020.0\n",
      "IceHockeyDeterministic-v4 ,  Discrete(18) ,  3273.9 ,  -7.6 ,  1.0\n",
      "JamesbondDeterministic-v4 ,  Discrete(18) ,  491.1 ,  45.0 ,  50.0\n",
      "JourneyEscapeDeterministic-v4 ,  Discrete(16) ,  733.0 ,  -19250.0 ,  0.0\n",
      "KangarooDeterministic-v4 ,  Discrete(18) ,  473.0 ,  120.0 ,  200.0\n",
      "KrullDeterministic-v4 ,  Discrete(18) ,  1389.2 ,  1498.4 ,  30.0\n",
      "KungFuMasterDeterministic-v4 ,  Discrete(14) ,  1034.4 ,  740.0 ,  500.0\n",
      "MontezumaRevengeDeterministic-v4 ,  Discrete(18) ,  543.0 ,  0.0 ,  0.0\n",
      "MsPacmanDeterministic-v4 ,  Discrete(9) ,  444.4 ,  241.0 ,  10.0\n",
      "NameThisGameDeterministic-v4 ,  Discrete(6) ,  2494.6 ,  2197.0 ,  150.0\n",
      "PhoenixDeterministic-v4 ,  Discrete(8) ,  1239.0 ,  647.0 ,  450.0\n",
      "PitfallDeterministic-v4 ,  Discrete(18) ,  2917.4 ,  -435.0 ,  0.0\n",
      "PongDeterministic-v4 ,  Discrete(6) ,  971.7 ,  -19.8 ,  1.0\n",
      "PooyanDeterministic-v4 ,  Discrete(6) ,  648.5 ,  356.0 ,  90.0\n",
      "PrivateEyeDeterministic-v4 ,  Discrete(18) ,  2693.0 ,  70.0 ,  100.0\n",
      "QbertDeterministic-v4 ,  Discrete(6) ,  299.5 ,  102.5 ,  25.0\n",
      "RiverraidDeterministic-v4 ,  Discrete(18) ,  749.0 ,  1596.0 ,  500.0\n",
      "RoadRunnerDeterministic-v4 ,  Discrete(18) ,  180.8 ,  0.0 ,  0.0\n",
      "RobotankDeterministic-v4 ,  Discrete(18) ,  2448.3 ,  2.9 ,  1.0\n",
      "SeaquestDeterministic-v4 ,  Discrete(18) ,  534.7 ,  96.0 ,  20.0\n",
      "SkiingDeterministic-v4 ,  Discrete(3) ,  1202.5 ,  -16200.6 ,  -6.0\n",
      "SolarisDeterministic-v4 ,  Discrete(18) ,  5773.3 ,  1770.0 ,  500.0\n",
      "SpaceInvadersDeterministic-v4 ,  Discrete(6) ,  628.0 ,  155.0 ,  200.0\n",
      "StarGunnerDeterministic-v4 ,  Discrete(18) ,  979.1 ,  600.0 ,  200.0\n",
      "TennisDeterministic-v4 ,  Discrete(18) ,  1639.2 ,  -23.9 ,  1.0\n",
      "TimePilotDeterministic-v4 ,  Discrete(10) ,  1677.4 ,  2570.0 ,  3100.0\n",
      "TutankhamDeterministic-v4 ,  Discrete(8) ,  645.5 ,  6.7 ,  20.0\n",
      "UpNDownDeterministic-v4 ,  Discrete(6) ,  503.9 ,  742.0 ,  600.0\n",
      "VentureDeterministic-v4 ,  Discrete(18) ,  2098.4 ,  0.0 ,  0.0\n",
      "VideoPinballDeterministic-v4 ,  Discrete(9) ,  3150.3 ,  25692.0 ,  4000.0\n",
      "WizardOfWorDeterministic-v4 ,  Discrete(10) ,  1969.8 ,  960.0 ,  500.0\n",
      "YarsRevengeDeterministic-v4 ,  Discrete(18) ,  955.6 ,  3098.1 ,  345.0\n",
      "ZaxxonDeterministic-v4 ,  Discrete(18) ,  882.0 ,  0.0 ,  0.0\n"
     ]
    }
   ],
   "source": [
    "from gym import envs\n",
    "all_envs = list(envs.registry.all())\n",
    "\n",
    "for env in all_envs:\n",
    "    if ('v4' in env.id):\n",
    "        m = env.make()\n",
    "        if (m.observation_space.shape[0] >= 200) and ('Deterministic' in env.id):\n",
    "            n = 0      \n",
    "            r = 0\n",
    "            max_r = -100000\n",
    "            for i in range(0, 10):\n",
    "                observation = m.reset()\n",
    "                for _ in range(0, 5):\n",
    "                    m.step(1)\n",
    "                j = 0    \n",
    "                while True:\n",
    "                    action = m.action_space.sample()\n",
    "                    observation, reward, done, info = m.step(action)\n",
    "                    n += 1\n",
    "                    r += reward\n",
    "                    j += 1\n",
    "                    max_r = np.max((reward, max_r))\n",
    "                    if done or (j>6000):\n",
    "                        break                    \n",
    "                    \n",
    "            m.close()\n",
    "            print(env.id, ', ', m.action_space, ', ', n/10.0, ', ', r/10.0, ', ', max_r)\n",
    "\n",
    "\n",
    "i=0\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(18)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.action_space"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
