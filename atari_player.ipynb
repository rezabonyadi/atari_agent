{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import tensorflow as tf\n",
    "from skimage.transform import resize\n",
    "from skimage.color import rgb2gray\n",
    "\n",
    "class Atari:\n",
    "    \"\"\"Wrapper for the environment provided by gym\"\"\"\n",
    "\n",
    "    def __init__(self, env_name, frame_height, frame_width, agent_history_length=4, no_op_steps=10):\n",
    "        self.env_name = env_name\n",
    "        self.env = gym.make(env_name)\n",
    "        print(\"The environment has the following {} actions: {}\".format(self.env.action_space.n,\n",
    "                                                                        self.env.unwrapped.get_action_meanings()))\n",
    "\n",
    "        # self.frame_processor = ProcessFrame()\n",
    "        self.current_state = np.empty((frame_height, frame_width, agent_history_length), dtype=np.uint8)\n",
    "        self.last_lives = 0\n",
    "        self.no_op_steps = no_op_steps\n",
    "        self.agent_history_length = agent_history_length\n",
    "        self.action_space_size = self.env.action_space.n\n",
    "        self.game_shape = self.env.observation_space.shape\n",
    "        self.is_graphical = True if len(self.game_shape) > 1 else False\n",
    "        self.frame_height = frame_height if self.is_graphical else self.game_shape[0]\n",
    "        self.frame_width = frame_width if self.is_graphical else 1\n",
    "\n",
    "    def reset(self, evaluation=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            evaluation: A boolean saying whether the agent is evaluating or training\n",
    "        Resets the environment and stacks four frames ontop of each other to\n",
    "        create the first state\n",
    "        \"\"\"\n",
    "        frame = self.env.reset()\n",
    "        self.last_lives = 0\n",
    "        terminal_life_lost = True  # Set to true so that the agent starts\n",
    "        # with a 'FIRE' action when evaluating\n",
    "        if evaluation:\n",
    "            for _ in range(np.random.randint(1, self.no_op_steps)):\n",
    "                frame, _, _, _ = self.env.step(1)  # Action 'Fire'\n",
    "        else:\n",
    "            frame, _, _, _ = self.env.step(1)\n",
    "\n",
    "        processed_frame = self.process(frame)\n",
    "        for i in range(self.agent_history_length):\n",
    "            self.update_current_state(processed_frame)\n",
    "\n",
    "        return terminal_life_lost\n",
    "\n",
    "    def process(self, frame):\n",
    "        # returns a (height, width, 1) array\n",
    "        if self.is_graphical:\n",
    "            # frame = frame[34:200, 0:160, :]\n",
    "            frame = resize(frame, (self.frame_height, self.frame_width))\n",
    "            frame = rgb2gray(frame)\n",
    "            frame = np.uint8(frame*255)\n",
    "        return frame\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            action: Integer, action the agent performs\n",
    "        Performs an action and observes the reward and terminal state from the environment\n",
    "        \"\"\"\n",
    "        new_frame, reward, terminal, info = self.env.step(action)  # (5â˜…)\n",
    "\n",
    "        if info['ale.lives'] < self.last_lives:\n",
    "            terminal_life_lost = True\n",
    "        else:\n",
    "            terminal_life_lost = terminal\n",
    "        self.last_lives = info['ale.lives']\n",
    "\n",
    "        processed_new_frame = self.process(new_frame)\n",
    "        self.update_current_state(processed_new_frame)\n",
    "\n",
    "        return processed_new_frame, reward, terminal, terminal_life_lost, new_frame\n",
    "\n",
    "    def get_current_state(self):\n",
    "        return self.current_state\n",
    "\n",
    "    def update_current_state(self, frame):\n",
    "        self.current_state = np.append(self.current_state[:, :, 1:], np.expand_dims(frame, axis=2), axis=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras import layers, callbacks\n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.core import Dense, Flatten, Lambda\n",
    "from keras.layers import Activation, Input\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.merge import Add\n",
    "from keras.initializers import VarianceScaling\n",
    "# from numba import *\n",
    "\n",
    "class QLearner:\n",
    "    def __init__(self, n_actions, learning_rate=0.00001,\n",
    "                 frame_height=84, frame_width=84, agent_history_length=4,\n",
    "                 batch_size=32, gamma=0.99, use_double_model=True):\n",
    "        self.n_actions = n_actions\n",
    "        self.learning_rate = learning_rate\n",
    "        self.frame_height = frame_height\n",
    "        self.frame_width = frame_width\n",
    "        self.agent_history_length = agent_history_length\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.use_double_model = use_double_model\n",
    "        # self.punishment = punishment\n",
    "\n",
    "        self.main_learner = DQN(self.n_actions, self.learning_rate,\n",
    "                                self.frame_height, self.frame_width, agent_history_length)\n",
    "\n",
    "        self.target_learner = DQN(self.n_actions, learning_rate,\n",
    "                                  self.frame_height, self.frame_width, agent_history_length)\n",
    "\n",
    "        self.targets = np.zeros((batch_size,))\n",
    "        self.set_computation_device()\n",
    "\n",
    "        # self.tbCallBack = [callbacks.TensorBoard(log_dir='./output/Tensorboards', histogram_freq=0, write_graph=True, write_images=True)]\n",
    "        self.tbCallBack = None\n",
    "\n",
    "    @staticmethod\n",
    "    def set_computation_device():\n",
    "        num_cores = 14\n",
    "        GPU = True\n",
    "\n",
    "        if GPU:\n",
    "            num_GPU = 1\n",
    "            num_CPU = 1\n",
    "        else:\n",
    "            num_CPU = 1\n",
    "            num_GPU = 0\n",
    "\n",
    "        config = tf.ConfigProto(intra_op_parallelism_threads=num_cores,\n",
    "                                inter_op_parallelism_threads=num_cores,\n",
    "                                allow_soft_placement=True,\n",
    "                                device_count={'CPU': num_CPU,\n",
    "                                              'GPU': num_GPU}\n",
    "                                )\n",
    "\n",
    "        session = tf.Session(config=config)\n",
    "        K.set_session(session)\n",
    "\n",
    "    # @jit\n",
    "    def predict(self, states):\n",
    "        actions_mask = np.ones((states.shape[0], self.n_actions))\n",
    "        return self.main_learner.model.predict([states, actions_mask])  # separate old model to predict\n",
    "\n",
    "    # @jit\n",
    "    def train(self, current_state_batch, actions, rewards, next_state_batch, terminal_flags):\n",
    "\n",
    "        self.calculate_target_q_values(next_state_batch, terminal_flags, rewards)\n",
    "\n",
    "        one_hot_actions = np.eye(self.n_actions)[np.array(actions).reshape(-1)]\n",
    "        one_hot_targets = one_hot_actions * self.targets[:, None]\n",
    "\n",
    "        history = self.main_learner.model.fit([current_state_batch, one_hot_actions], one_hot_targets,\n",
    "                                 epochs=1, batch_size=self.batch_size, verbose=0, callbacks=self.tbCallBack)\n",
    "\n",
    "        return history.history['loss'][0]\n",
    "\n",
    "    # @jit\n",
    "    def update_target_network(self):\n",
    "        if self.use_double_model:\n",
    "            print('Updating the target network')\n",
    "            self.target_learner.model.set_weights(self.main_learner.model.get_weights())\n",
    "        else:\n",
    "            print('Doubling is off, no need to update target network')\n",
    "\n",
    "    # @jit\n",
    "    def calculate_target_q_values(self, next_state_batch, terminal_flags, rewards):\n",
    "        actions_mask = np.ones((self.batch_size, self.n_actions))\n",
    "        q_next_state = self.main_learner.model.predict([next_state_batch, actions_mask])  # separate old model to predict\n",
    "        action, _ = self.action_selection_policy(q_next_state)\n",
    "        if self.use_double_model:\n",
    "            q_target = self.target_learner.model.predict([next_state_batch, actions_mask])  # separate old model to predict\n",
    "        else:\n",
    "            q_target = q_next_state\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            if terminal_flags[i]:\n",
    "                self.targets[i] = rewards[i]\n",
    "            else:\n",
    "                self.targets[i] = rewards[i] + self.gamma * q_target[i, action[i]]\n",
    "\n",
    "    # @jit\n",
    "    def action_selection_policy(self, q_values):\n",
    "        # v = q_values - q_values.min(axis=1).reshape((-1, 1))\n",
    "        # v += 1.0\n",
    "        # sums = v.sum(axis=1).reshape((-1, 1))\n",
    "        # v = v / sums\n",
    "        # v = np.cumsum(v, axis=1)\n",
    "        #\n",
    "        # res = np.empty(q_values.shape[0], dtype=np.int32)\n",
    "        # r = np.random.rand(q_values.shape[0])\n",
    "        # for i in range(q_values.shape[0]):\n",
    "        #     res[i] = np.argwhere(v[i,:] >= r[i])[0,0]\n",
    "\n",
    "        res = np.argmax(q_values, axis=1)\n",
    "        return res, q_values[0,res][0]\n",
    "\n",
    "class DQN:\n",
    "\n",
    "    def __init__(self, n_actions, learning_rate=0.00001,\n",
    "                 frame_height=84, frame_width=84, agent_history_length=4):\n",
    "        self.n_actions = n_actions\n",
    "        self.learning_rate = learning_rate\n",
    "        self.frame_height = frame_height\n",
    "        self.frame_width = frame_width\n",
    "        self.agent_history_length = agent_history_length\n",
    "\n",
    "        input_shape = (frame_height, frame_width, agent_history_length)\n",
    "        # model = self.legacy_model(input_shape, self.n_actions)\n",
    "        # model = self.dueling_convnet(input_shape, self.n_actions)\n",
    "        # model = self.my_convnet(input_shape, self.n_actions)\n",
    "        model = self.nature_convnet(input_shape, self.n_actions)\n",
    "        # model = self.small_nature_convnet(input_shape, self.n_actions)\n",
    "        # model = self.sim_nature_convnet(input_shape, self.n_actions)\n",
    "        # model = self.modular_convnet(input_shape, self.n_actions)\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        optimizer = RMSprop(lr=self.learning_rate, rho=0.95)\n",
    "        # optimizer = Adam(lr=self.learning_rate)\n",
    "        model.compile(optimizer, loss=tf.losses.huber_loss)\n",
    "\n",
    "        self.model = model\n",
    "\n",
    "\n",
    "    def huber_loss(self, y, q_value):\n",
    "        error = K.abs(y - q_value)\n",
    "        quadratic_part = K.clip(error, 0.0, 1.0)\n",
    "        linear_part = error - quadratic_part\n",
    "        loss = K.mean(0.5 * K.square(quadratic_part) + linear_part)\n",
    "        return loss\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def legacy_model(input_shape, num_actions):\n",
    "        frames_input = layers.Input(input_shape, name='inputs')\n",
    "        actions_input = layers.Input((num_actions,), name='action_mask')\n",
    "\n",
    "        normalized = layers.Lambda(lambda x: x / 255.0, name='norm')(frames_input)\n",
    "\n",
    "        conv_1 = layers.convolutional.Conv2D(\n",
    "            64, (8, 8), strides=(2, 2), activation='relu', kernel_initializer='VarianceScaling')(normalized)\n",
    "        conv_2 = layers.convolutional.Conv2D(\n",
    "            32, (4, 4), strides=(2, 2), activation='relu', kernel_initializer='VarianceScaling')(conv_1)\n",
    "        conv_3 = layers.convolutional.Conv2D(\n",
    "            32, (3, 3), strides=(1, 1), activation='relu', kernel_initializer='VarianceScaling')(conv_2)\n",
    "        conv_4 = layers.convolutional.Conv2D(\n",
    "            32, (7, 7), strides=(1, 1), activation='relu', kernel_initializer='VarianceScaling')(conv_3)\n",
    "\n",
    "        conv_flattened = layers.core.Flatten()(conv_4)\n",
    "        hidden = layers.Dense(256, activation='relu')(conv_flattened)\n",
    "        output = layers.Dense(num_actions)(hidden)\n",
    "\n",
    "        filtered_output = layers.Multiply(name='QValue')([output, actions_input])\n",
    "\n",
    "        model = Model(inputs=[frames_input, actions_input], outputs=filtered_output)\n",
    "\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def linear(input_shape, num_actions):\n",
    "        model = Sequential()\n",
    "        model.add(Flatten(\n",
    "            input_shape=input_shape))\n",
    "        model.add(Dense(\n",
    "            num_actions,\n",
    "            activation=None))\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def convnet(input_shape, num_actions):\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(16, 8, strides=(4, 4), activation='relu', input_shape=input_shape))\n",
    "        model.add(Conv2D(32, 4, strides=(2, 2), activation='relu'))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(256, activation='relu'))\n",
    "        model.add(Dense(num_actions, activation=None))\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def convnet_bn(input_shape, num_actions):\n",
    "        frames_input = Input(shape=input_shape)\n",
    "        normalized = layers.Lambda(lambda x: x / 255.0, name='norm')(frames_input)\n",
    "        net = Conv2D(16, 8, strides=(4, 4), activation='relu')(normalized)\n",
    "        net = Conv2D(32, 4, strides=(2, 2), activation='relu')(net)\n",
    "        net = Flatten()(net)\n",
    "        net = Dense(32, activation='relu')(net)\n",
    "        net = BatchNormalization()(net)\n",
    "        net = Dense(num_actions, activation=None)(net)\n",
    "        model = DQN.add_action_mask_layer(net, frames_input, num_actions)\n",
    "\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def simpler_convnet(input_shape, num_actions):\n",
    "        frames_input = Input(shape=input_shape)\n",
    "        normalized = layers.Lambda(lambda x: x / 255.0, name='norm')(frames_input)\n",
    "        net = Conv2D(16, 8, strides=(4, 4), activation='relu')(normalized)\n",
    "        net = Conv2D(32, 4, strides=(2, 2), activation='relu')(net)\n",
    "        net = Flatten()(net)\n",
    "        net = Dense(32, activation='relu')(net)\n",
    "        net = Dense(num_actions, activation=None)(net)\n",
    "        model = DQN.add_action_mask_layer(net, frames_input, num_actions)\n",
    "\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def nature_convnet(input_shape, num_actions):\n",
    "        frames_input = Input(shape=input_shape)\n",
    "        normalized = layers.Lambda(lambda x: x / 255.0, name='norm')(frames_input)\n",
    "        net = Conv2D(32, 8, strides=(4, 4), activation='relu')(normalized)\n",
    "        net = Conv2D(64, 4, strides=(2, 2), activation='relu')(net)\n",
    "        net = Conv2D(64, 3, strides=(1, 1), activation='relu')(net)\n",
    "        net = Flatten()(net)\n",
    "        net = Dense(512, activation='relu')(net)\n",
    "        net = Dense(num_actions, activation=None)(net)\n",
    "        model = DQN.add_action_mask_layer(net, frames_input, num_actions)\n",
    "\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def small_nature_convnet(input_shape, num_actions):\n",
    "        frames_input = Input(shape=input_shape)\n",
    "        normalized = layers.Lambda(lambda x: x / 255.0, name='norm')(frames_input)\n",
    "        net = Conv2D(8, 8, strides=(4, 4), activation='relu')(normalized)\n",
    "        net = Conv2D(16, 4, strides=(2, 2), activation='relu')(net)\n",
    "        net = Conv2D(16, 3, strides=(1, 1), activation='relu')(net)\n",
    "        net = Flatten()(net)\n",
    "        net = Dense(512, activation='relu')(net)\n",
    "        net = Dense(num_actions, activation=None)(net)\n",
    "        model = DQN.add_action_mask_layer(net, frames_input, num_actions)\n",
    "\n",
    "        return model\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def dueling_convnet(input_shape, num_actions):\n",
    "        initializer = VarianceScaling(scale=2.0)\n",
    "        frames_input = Input(shape=input_shape)\n",
    "        normalized = layers.Lambda(lambda x: x / 255.0, name='norm')(frames_input)\n",
    "\n",
    "        net = Conv2D(32, (8, 8), strides=(4, 4), activation='relu', kernel_initializer=initializer, padding='valid',\n",
    "                     use_bias=False)(normalized)\n",
    "        net = Conv2D(64, (4, 4), strides=(2, 2), activation='relu', kernel_initializer=initializer, padding='valid',\n",
    "                     use_bias=False)(net)\n",
    "        net = Conv2D(64, (3, 3), strides=(1, 1), activation='relu', kernel_initializer=initializer, padding='valid',\n",
    "                     use_bias=False)(net)\n",
    "        net = Conv2D(1024, (7, 7), strides=(1, 1), activation='relu', kernel_initializer=initializer, padding='valid',\n",
    "                     use_bias=False)(net)\n",
    "\n",
    "        net = Flatten()(net)\n",
    "        advt = Dense(256, kernel_initializer=initializer)(net)\n",
    "        # advt = Dense(50, kernel_initializer=initializer)(net)\n",
    "\n",
    "        advt = Dense(num_actions)(advt)\n",
    "        value = Dense(256, kernel_initializer=initializer)(net)\n",
    "        # value = Dense(50, kernel_initializer=initializer)(net)\n",
    "\n",
    "        value = Dense(1)(value)\n",
    "        # now to combine the two streams\n",
    "        advt = Lambda(lambda advt: advt - tf.reduce_mean(advt, axis=-1, keep_dims=True))(advt)\n",
    "        value = Lambda(lambda value: tf.tile(value, [1, num_actions]))(value)\n",
    "        final = Add()([value, advt])\n",
    "\n",
    "        model = DQN.add_action_mask_layer(final, frames_input, num_actions)\n",
    "\n",
    "        # model = Model(inputs=inputs, outputs=final)\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def my_convnet(input_shape, num_actions):\n",
    "        initializer = VarianceScaling(scale=2.0)\n",
    "        frames_input = Input(shape=input_shape)\n",
    "        normalized = layers.Lambda(lambda x: x / 255.0, name='norm')(frames_input)\n",
    "\n",
    "        net = Conv2D(32, (8, 8), strides=(4, 4),\n",
    "                     activation='relu', kernel_initializer=initializer,\n",
    "                     padding='valid', use_bias=False)(normalized)\n",
    "        net = Conv2D(64, (4, 4), strides=(2, 2),\n",
    "                     activation='relu', kernel_initializer=initializer,\n",
    "                     padding='valid', use_bias=False)(net)\n",
    "        net = Conv2D(64, (4, 4), strides=(1, 1),\n",
    "                     activation='relu', kernel_initializer=initializer,\n",
    "                     padding='valid', use_bias=False)(net)\n",
    "        net = Conv2D(64, (4, 4), strides=(1, 1),\n",
    "                     activation='relu', kernel_initializer=initializer,\n",
    "                     padding='valid', use_bias=False)(net)\n",
    "        net = Conv2D(128, (3, 3), strides=(1, 1),\n",
    "                     activation='relu', kernel_initializer=initializer,\n",
    "                     padding='valid', use_bias=False)(net)\n",
    "\n",
    "        net = Flatten()(net)\n",
    "        advt = Dense(32, kernel_initializer=initializer)(net)\n",
    "        # advt = Dense(50, kernel_initializer=initializer)(net)\n",
    "\n",
    "        advt = Dense(num_actions)(advt)\n",
    "        value = Dense(32, kernel_initializer=initializer)(net)\n",
    "        # value = Dense(50, kernel_initializer=initializer)(net)\n",
    "\n",
    "        value = Dense(1)(value)\n",
    "        # now to combine the two streams\n",
    "        advt = Lambda(lambda advt: advt - tf.reduce_mean(advt, axis=-1, keep_dims=True))(advt)\n",
    "        value = Lambda(lambda value: tf.tile(value, [1, num_actions]))(value)\n",
    "        final = Add()([value, advt])\n",
    "\n",
    "        model = DQN.add_action_mask_layer(final, frames_input, num_actions)\n",
    "\n",
    "        # model = Model(inputs=inputs, outputs=final)\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def modular_convnet(input_shape, num_actions):\n",
    "        frames_input = Input(shape=input_shape)\n",
    "        normalized = layers.Lambda(lambda x: x / 255.0, name='norm')(frames_input)\n",
    "        # Vision\n",
    "        net = Conv2D(32, 8, strides=(4, 4), activation='relu')(normalized)\n",
    "        net = Conv2D(64, 4, strides=(2, 2), activation='relu')(net)\n",
    "        net = Conv2D(64, 3, strides=(1, 1), activation='relu')(net)\n",
    "        net = Flatten()(net)\n",
    "\n",
    "        # Reasoning\n",
    "        net = Dense(256, activation='tanh')(net)\n",
    "        net = Dense(64, activation='tanh')(net)\n",
    "\n",
    "        # Action decision maker\n",
    "        net = Dense(32, activation='relu')(net)\n",
    "        net = Dense(num_actions, activation=None)(net)\n",
    "\n",
    "        model = DQN.add_action_mask_layer(net, frames_input, num_actions)\n",
    "\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def sim_nature_convnet(input_shape, num_actions):\n",
    "        frames_input = Input(shape=input_shape)\n",
    "        normalized = layers.Lambda(lambda x: x / 255.0, name='norm')(frames_input)\n",
    "        # net = Conv2D(32, 8, strides=(4, 4), activation='relu')(normalized)\n",
    "        # net = Conv2D(64, 4, strides=(2, 2), activation='relu')(net)\n",
    "        # net = Conv2D(64, 3, strides=(1, 1), activation='relu')(net)\n",
    "\n",
    "        net = Conv2D(64, 16, strides=(10, 10), activation='relu')(normalized)\n",
    "\n",
    "        net = Flatten()(net)\n",
    "        net = Dense(512, activation='relu')(net)\n",
    "        net = Dense(num_actions, activation=None)(net)\n",
    "        model = DQN.add_action_mask_layer(net, frames_input, num_actions)\n",
    "\n",
    "        return model\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def add_action_mask_layer(final, frames_input, num_actions):\n",
    "        actions_input = layers.Input((num_actions,), name='action_mask')\n",
    "        filtered_output = layers.Multiply(name='QValue')([final, actions_input])\n",
    "        model = Model(inputs=[frames_input, actions_input], outputs=filtered_output)\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "# from memory import ReplayMemory\n",
    "# from learner import QLearner\n",
    "from keras.models import model_from_json\n",
    "# from numba import *\n",
    "\n",
    "\n",
    "class Player:\n",
    "    def __init__(self, game_env, agent_history_length, total_memory_size, batch_size,\n",
    "                 learning_rate, init_epsilon, end_epsilon, minimum_observe_episode,\n",
    "                 update_target_frequency, train_frequency, gamma, exploratory_memory_size,\n",
    "                 punishment, reward_extrapolation_exponent, linear_exploration_exponent, use_double):\n",
    "\n",
    "        self.n_actions = game_env.action_space_size\n",
    "        self.init_epsilon = init_epsilon\n",
    "        self.epsilon = init_epsilon\n",
    "        self.end_epsilon = end_epsilon\n",
    "        self.minimum_observe_episodes = minimum_observe_episode\n",
    "        self.update_target_frequency = update_target_frequency\n",
    "        self.game_env = game_env\n",
    "        self.train_frequency = train_frequency\n",
    "        self.exploratory_memory_size = exploratory_memory_size\n",
    "        self.linear_exploration_exponent = linear_exploration_exponent\n",
    "        self.use_double_model = use_double\n",
    "\n",
    "        if reward_extrapolation_exponent < 0:\n",
    "            use_estimated_reward = False\n",
    "        else:\n",
    "            use_estimated_reward = True\n",
    "\n",
    "        self.memory = ReplayMemory(self.game_env.frame_height, self.game_env.frame_width,\n",
    "                                   agent_history_length, total_memory_size,\n",
    "                                   batch_size, self.game_env.is_graphical,\n",
    "                                   punishment=punishment, use_estimated_reward=use_estimated_reward,\n",
    "                                   reward_extrapolation_exponent=reward_extrapolation_exponent,\n",
    "                                   linear_exploration_exponent=self.linear_exploration_exponent)\n",
    "\n",
    "        self.learner = QLearner(self.n_actions, learning_rate, self.game_env.frame_height, self.game_env.frame_width,\n",
    "                                agent_history_length, gamma=gamma, use_double_model=self.use_double_model)\n",
    "        self.losses = []\n",
    "        self.q_values = []\n",
    "\n",
    "        # self.actuator = ???\n",
    "\n",
    "    # @jit\n",
    "    def take_action(self, current_state, total_frames, evaluation=False):\n",
    "        if (np.random.rand() <= self.epsilon) or (total_frames < self.exploratory_memory_size) and (not evaluation):\n",
    "            action = np.random.randint(0, self.n_actions)\n",
    "        else:\n",
    "            current_state = np.expand_dims(current_state, axis=0)\n",
    "            q_values = self.learner.predict(current_state)\n",
    "\n",
    "            action, q_value = self.learner.action_selection_policy(q_values)\n",
    "            self.q_values.append(q_value)\n",
    "\n",
    "        return action\n",
    "\n",
    "    # @jit\n",
    "    def learn(self, no_passed_frames):\n",
    "        if no_passed_frames % self.train_frequency == 0:\n",
    "            current_state_batch, actions, rewards, next_state_batch, terminal_flags = self.memory.get_minibatch()\n",
    "            loss = self.learner.train(current_state_batch, actions, rewards, next_state_batch, terminal_flags)\n",
    "            self.losses.append(loss)\n",
    "\n",
    "        if no_passed_frames % self.update_target_frequency == 0:\n",
    "            self.learner.update_target_network()\n",
    "\n",
    "    # @jit\n",
    "    def updates(self, no_passed_frames, episode, action, processed_new_frame, reward, terminal_life_lost, episode_seq):\n",
    "        self.memory.add_experience(action, processed_new_frame, reward, terminal_life_lost, episode_seq, episode)\n",
    "\n",
    "        if no_passed_frames > self.exploratory_memory_size:\n",
    "            self.update_epsilon(episode)\n",
    "            self.learn(no_passed_frames)\n",
    "\n",
    "    # @jit\n",
    "    def update_epsilon(self, episode):\n",
    "        self.epsilon -= 0.00001\n",
    "        self.epsilon = max(self.epsilon, self.end_epsilon)\n",
    "        # print('Epsilon: ', str(self.epsilon))\n",
    "\n",
    "    # @jit\n",
    "    def save_player_learner(self, folder):\n",
    "        model_json = self.learner.main_learner.model.to_json(indent=4)\n",
    "        with open(''.join([folder, 'model_structure.jsn']), \"w\") as json_file:\n",
    "            json_file.write(model_json)\n",
    "\n",
    "        self.learner.main_learner.model.save_weights(''.join([folder, 'model_weights.wts']))\n",
    "\n",
    "    # @jit\n",
    "    def load_player_learner(self, folder):\n",
    "        json_file = open(''.join([folder, 'model_structure.jsn']), 'r')\n",
    "        loaded_model_json = json_file.read()\n",
    "        json_file.close()\n",
    "        loaded_model = model_from_json(loaded_model_json)\n",
    "        loaded_model.load_weights(''.join([folder,'model_weights.wts']))\n",
    "\n",
    "        self.learner.main_learner.model = loaded_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from player.player import Player\n",
    "from environments.simulator import Atari\n",
    "import numpy as np\n",
    "import datetime\n",
    "import os\n",
    "import json\n",
    "import csv\n",
    "\n",
    "class HandleResults:\n",
    "\n",
    "    folder_to_use = ''\n",
    "    settings_file_name = 'settings.jsn'\n",
    "    time = datetime.datetime.now()\n",
    "\n",
    "    def __init__(self, game_env, out_folder):\n",
    "        # GAME_ENV = settings['GAME_ENV']\n",
    "        d = datetime.datetime.now().strftime(\"%y_%m_%d_%H_%M_%S\")\n",
    "        self.folder_to_use = ''.join([out_folder, game_env, '/results_', d, '/'])\n",
    "        os.makedirs(self.folder_to_use)\n",
    "        self.results_file_name = ''.join([self.folder_to_use, 'results.csv'])\n",
    "\n",
    "    def save_settings(self, settings, player):\n",
    "        settings_dict = settings\n",
    "\n",
    "        with open(''.join([self.folder_to_use, 'settings.jsn']), 'wt') as outfile:\n",
    "            json.dump(settings_dict, outfile, indent=4)\n",
    "\n",
    "        player.save_player_learner(self.folder_to_use)\n",
    "\n",
    "    def load_default_settings(self, GAME_ENV):\n",
    "\n",
    "        settings_dict = {}\n",
    "        file_name = './default_settings.jsn'  # default_settings.jsn i in the root\n",
    "        with open(file_name, 'rt') as json_file:\n",
    "            settings_dict = json.load(json_file)\n",
    "        settings_dict['GAME_ENV'] = GAME_ENV\n",
    "        game_env = Atari(settings_dict['GAME_ENV'], settings_dict['frame_height'], settings_dict['frame_width'],\n",
    "                         agent_history_length=settings_dict['AGENT_HISTORY_LENGTH'],\n",
    "                         no_op_steps=settings_dict['NO_OP_STEPS'])\n",
    "\n",
    "        player = self.build_player(settings_dict, game_env)\n",
    "\n",
    "        return player, game_env, settings_dict['MAX_EPISODE_LENGTH'], settings_dict['MAX_EPISODES'], settings_dict\n",
    "\n",
    "    def load_default_settings_constants(self, GAME_ENV):\n",
    "\n",
    "        settings_dict = {}\n",
    "        file_name = './default_settings.jsn'  # default_settings.jsn i in the root\n",
    "        with open(file_name, 'rt') as json_file:\n",
    "            settings_dict = json.load(json_file)\n",
    "\n",
    "        settings_dict['GAME_ENV'] = GAME_ENV\n",
    "        settings_dict['AGENT_HISTORY_LENGTH'] = AGENT_HISTORY_LENGTH\n",
    "        settings_dict['MEMORY_SIZE'] = MEMORY_SIZE\n",
    "        settings_dict['BS'] = BS\n",
    "        settings_dict['LEARNING_RATE']=LEARNING_RATE\n",
    "        settings_dict['INI_EPSILON'] = INI_EPSILON\n",
    "        settings_dict['END_EPSILON'] = END_EPSILON\n",
    "        settings_dict['MIN_OBSERVE_EPISODE'] = MIN_OBSERVE_EPISODE\n",
    "        settings_dict['NETW_UPDATE_FREQ'] =NETW_UPDATE_FREQ\n",
    "        settings_dict['UPDATE_FREQ'] = UPDATE_FREQ\n",
    "        settings_dict['DISCOUNT_FACTOR'] = DISCOUNT_FACTOR\n",
    "        settings_dict['REPLAY_MEMORY_START_SIZE'] = REPLAY_MEMORY_START_SIZE\n",
    "        settings_dict['PUNISH'] = PUNISH\n",
    "        settings_dict['REWARD_EXTRAPOLATION_EXPONENT'] = REWARD_EXTRAPOLATION_EXPONENT\n",
    "        settings_dict['LINEAR_EXPLORATION_EXPONENT'] = LINEAR_EXPLORATION_EXPONENT\n",
    "        settings_dict['USE_DOUBLE_MODEL'] = USE_DOUBLE_MODEL\n",
    "\n",
    "        game_env = Atari(settings_dict['GAME_ENV'], settings_dict['frame_height'], settings_dict['frame_width'],\n",
    "                         agent_history_length=settings_dict['AGENT_HISTORY_LENGTH'],\n",
    "                         no_op_steps=settings_dict['NO_OP_STEPS'])\n",
    "\n",
    "        player = self.build_player(settings_dict, game_env)\n",
    "\n",
    "        return player, game_env, settings_dict['MAX_EPISODE_LENGTH'], settings_dict['MAX_EPISODES'], settings_dict\n",
    "\n",
    "    def load_settings(self, folder, load_model):\n",
    "        settings_dict = {}\n",
    "        with open(''.join([folder, self.settings_file_name]), 'rt') as json_file:\n",
    "            settings_dict = json.load(json_file)\n",
    "\n",
    "        game_env = Atari(settings_dict['GAME_ENV'], settings_dict['frame_height'], settings_dict['frame_width'],\n",
    "                         agent_history_length=settings_dict['AGENT_HISTORY_LENGTH'], no_op_steps=settings_dict['NO_OP_STEPS'])\n",
    "\n",
    "        player = self.build_player(settings_dict, game_env)\n",
    "\n",
    "        if load_model:\n",
    "            player.load_player_learner(folder)\n",
    "\n",
    "        return player, game_env, settings_dict['MAX_EPISODE_LENGTH'], settings_dict['MAX_EPISODES'], settings_dict\n",
    "\n",
    "    def save_res(self, res_dict):\n",
    "        if not os.path.isfile(self.results_file_name):\n",
    "            with open(self.results_file_name, mode='w', newline='') as file:\n",
    "                writer = csv.writer(file)\n",
    "                headings = list(res_dict.keys())\n",
    "                writer.writerow(headings)\n",
    "            file.close()\n",
    "\n",
    "        with open(self.results_file_name, mode='a', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            values = list(res_dict.values())\n",
    "            writer.writerow(values)\n",
    "        file.close()\n",
    "\n",
    "        print(res_dict)\n",
    "\n",
    "    def build_player(self, settings_dict, game_env):\n",
    "        player = Player(game_env, settings_dict['AGENT_HISTORY_LENGTH'], settings_dict['MEMORY_SIZE'],\n",
    "                        settings_dict['BS'],\n",
    "                        settings_dict['LEARNING_RATE'], settings_dict['INI_EPSILON'], settings_dict['END_EPSILON'],\n",
    "                        settings_dict['MIN_OBSERVE_EPISODE'], settings_dict['NETW_UPDATE_FREQ'],\n",
    "                        settings_dict['UPDATE_FREQ'], settings_dict['DISCOUNT_FACTOR'],\n",
    "                        settings_dict['REPLAY_MEMORY_START_SIZE'], settings_dict['PUNISH'],\n",
    "                        settings_dict['REWARD_EXTRAPOLATION_EXPONENT'], settings_dict['LINEAR_EXPLORATION_EXPONENT'],\n",
    "                        settings_dict['USE_DOUBLE_MODEL'])\n",
    "        return player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(max_episode_length, episode, game_env, player, total_frames, evaluation=False):\n",
    "    terminal_life_lost = game_env.reset()\n",
    "    episode_reward = 0\n",
    "    life_seq = 0\n",
    "    frame_number = 0\n",
    "    gif_frames = []\n",
    "    while True:\n",
    "        # Get state, make action, get next state (rewards, terminal, ...), record the experience, train if necessary\n",
    "        current_state = game_env.get_current_state()\n",
    "        action = player.take_action(current_state, total_frames, evaluation)\n",
    "        processed_new_frame, reward, terminal, terminal_life_lost, original_frame = game_env.step(action)\n",
    "\n",
    "        if frame_number >= max_episode_length:\n",
    "            terminal = True\n",
    "            terminal_life_lost = True\n",
    "\n",
    "        # if evaluation:\n",
    "        #     gif_frames.append(original_frame)\n",
    "\n",
    "        if not evaluation:\n",
    "            player.updates(total_frames, episode, action, processed_new_frame, reward, terminal_life_lost, life_seq)\n",
    "\n",
    "        episode_reward += reward\n",
    "        life_seq += 1\n",
    "\n",
    "        if terminal_life_lost:\n",
    "            life_seq = 0\n",
    "\n",
    "        # game_env.env.render()\n",
    "        total_frames += 1\n",
    "        frame_number += 1\n",
    "\n",
    "        if terminal:\n",
    "            break\n",
    "\n",
    "    return episode_reward, total_frames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.core import Dense, Flatten, Lambda\n",
    "from keras.layers import Activation, Input\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.merge import Add\n",
    "from keras.initializers import VarianceScaling\n",
    "from keras import layers, callbacks\n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras import backend as K\n",
    "# from numba import *\n",
    "\n",
    "\n",
    "class ReplayMemory:\n",
    "\n",
    "    def __init__(self, frame_height, frame_width, agent_history_length=4, size=1000000, batch_size=32,\n",
    "                 is_graphical=True, use_spotlight=False, use_estimated_reward=True, punishment=0.0,\n",
    "                 reward_extrapolation_exponent=10.0, linear_exploration_exponent=True):\n",
    "\n",
    "        self.use_estimated_reward = use_estimated_reward\n",
    "        self.use_spotlight = use_spotlight\n",
    "        self.size = size\n",
    "        self.frame_height = frame_height\n",
    "        self.frame_width = frame_width\n",
    "        self.agent_history_length = agent_history_length\n",
    "        self.batch_size = batch_size\n",
    "        self.count = 0\n",
    "        self.current = 0\n",
    "        self.is_graphical = is_graphical\n",
    "        self.punishment_factor = punishment\n",
    "        self.reward_extrapolation_exponent = reward_extrapolation_exponent\n",
    "        self.linear_exploration_exponent = linear_exploration_exponent\n",
    "\n",
    "        self.actions = np.empty(self.size, dtype=np.int32)\n",
    "        self.rewards = np.empty(self.size, dtype=np.float32)\n",
    "        self.backfill_factor = np.empty(self.size, dtype=np.float32)\n",
    "        self.backfilled_reward = np.empty(self.size, dtype=np.float32)\n",
    "\n",
    "        if is_graphical:\n",
    "            self.frames = np.empty((self.size, self.frame_height, self.frame_width), dtype=np.uint8)\n",
    "        else:\n",
    "            self.frames = np.empty((self.size, self.frame_height), dtype=np.float16)\n",
    "        self.terminal_flags = np.empty(self.size, dtype=np.bool)\n",
    "        self.frame_number_in_epison = np.empty(self.size, dtype=np.int)\n",
    "        self.sparsity_lengths = []\n",
    "        self.min_reward = 0.0\n",
    "        self.max_reward = 0.0\n",
    "\n",
    "\n",
    "        if is_graphical:\n",
    "            self.minibatch_states = np.empty((self.batch_size, self.agent_history_length,\n",
    "                                    self.frame_height, self.frame_width), dtype=np.uint8)\n",
    "            self.minibatch_new_states = np.empty((self.batch_size, self.agent_history_length,\n",
    "                                        self.frame_height, self.frame_width), dtype=np.uint8)\n",
    "        else:\n",
    "            self.minibatch_states = np.empty((self.batch_size, self.agent_history_length,\n",
    "                                              self.frame_height), dtype=np.float16)\n",
    "            self.minibatch_new_states = np.empty((self.batch_size, self.agent_history_length,\n",
    "                                                  self.frame_height), dtype=np.float16)\n",
    "\n",
    "        self.minibatch_indices = np.empty(self.batch_size, dtype=np.int32)\n",
    "        self.minibatch_rewards = np.empty(self.batch_size, dtype=np.float32)\n",
    "\n",
    "        input_shape = (frame_height, frame_width, 1)\n",
    "\n",
    "        self.spotlight = SpotlightAttention(input_shape)\n",
    "\n",
    "    # @jit\n",
    "    def add_experience(self, action, frame, reward, terminal, frame_in_seq, episode):\n",
    "        self.min_reward = np.min((self.min_reward, reward))\n",
    "        self.max_reward = np.max((self.max_reward, reward))\n",
    "\n",
    "        if self.linear_exploration_exponent:\n",
    "            self.update_reward_exponent(episode)\n",
    "\n",
    "        if self.use_spotlight:\n",
    "            f = np.expand_dims(frame, axis=0)\n",
    "            f = np.expand_dims(f, axis=3)\n",
    "\n",
    "            seen_before = self.spotlight.seen_before(f)\n",
    "            self.spotlight.spotlight_train(f)\n",
    "        else:\n",
    "            seen_before = False\n",
    "\n",
    "        if not seen_before:\n",
    "            if terminal:\n",
    "                reward -= (self.punishment_factor*(self.max_reward + 1.0))\n",
    "                # reward -= self.punishment_factor\n",
    "\n",
    "            self.actions[self.current] = action\n",
    "            self.frames[self.current, ...] = frame\n",
    "            self.rewards[self.current] = reward\n",
    "            self.terminal_flags[self.current] = terminal\n",
    "            self.frame_number_in_epison[self.current] = frame_in_seq\n",
    "\n",
    "            if self.use_estimated_reward:\n",
    "                self.populate_reward_factors(reward)\n",
    "\n",
    "            self.count = max(self.count, self.current + 1)\n",
    "            self.current = (self.current + 1) % self.size\n",
    "\n",
    "    # @jit\n",
    "    def populate_reward_factors(self, current_reward):\n",
    "        if current_reward != 0:\n",
    "            prev_reward_indx = self.current - 1\n",
    "\n",
    "            while (self.frame_number_in_epison[prev_reward_indx] > 0) and (self.rewards[prev_reward_indx] == 0.0) \\\n",
    "                    and (prev_reward_indx > 0):\n",
    "                prev_reward_indx -= 1\n",
    "\n",
    "            start_indx = prev_reward_indx\n",
    "            end_indx = self.current\n",
    "            sparsity_length = end_indx - start_indx  # Length of consecutive zero rewards\n",
    "            self.sparsity_lengths.append(sparsity_length)\n",
    "\n",
    "            for i in range(start_indx, end_indx):\n",
    "                self.backfill_factor[i] = (i - start_indx) / sparsity_length\n",
    "                self.backfilled_reward[i] = current_reward\n",
    "\n",
    "            self.backfilled_reward[end_indx] = current_reward\n",
    "            self.backfill_factor[end_indx] = 1.0\n",
    "\n",
    "    def update_reward_exponent(self, episode):\n",
    "        s_episode = START_EPISODE\n",
    "        e_episode = END_EPISODE\n",
    "        s_exponent = START_EXPONENT\n",
    "        e_exponent = END_EXPONENT\n",
    "\n",
    "        if episode < s_episode:\n",
    "            self.reward_extrapolation_exponent = s_exponent\n",
    "        if episode > e_episode:\n",
    "            self.reward_extrapolation_exponent = e_exponent\n",
    "        if (episode >= s_episode) and (episode <= e_episode):\n",
    "            self.reward_extrapolation_exponent = \\\n",
    "                ((e_exponent-s_exponent)/(e_episode-s_episode))*(episode-s_episode)+s_exponent\n",
    "\n",
    "        if e_episode > IGNORE_EXPONENT_EPISODE:\n",
    "            self.use_estimated_reward = False\n",
    "\n",
    "    # # @jit\n",
    "    # def get_estimated_reward(self, recent_reward, sparsity_length, current_index):\n",
    "    #     # return recent_reward*np.power(current_index/sparsity_length, self.reward_extrapolation_exponent)\n",
    "    #     return current_index / sparsity_length\n",
    "\n",
    "    # @jit\n",
    "    def _get_state(self, index):\n",
    "        if self.count is 0:\n",
    "            raise ValueError(\"The replay memory is empty!\")\n",
    "        if index < self.agent_history_length - 1:\n",
    "            raise ValueError(\"Index must be min 3\")\n",
    "        return self.frames[index - self.agent_history_length + 1:index + 1, ...]\n",
    "\n",
    "    # @jit\n",
    "    def _get_valid_indices(self):\n",
    "        for i in range(self.batch_size):\n",
    "            while True:\n",
    "                index = np.random.randint(self.agent_history_length, self.count - 1)\n",
    "                if index < self.agent_history_length:\n",
    "                    continue\n",
    "                if index >= self.current and index - self.agent_history_length <= self.current:\n",
    "                    continue\n",
    "                # if self.terminal_flags[index - self.agent_history_length:index].any():\n",
    "                #     continue\n",
    "                if self.frame_number_in_epison[index] - self.frame_number_in_epison[index - self.agent_history_length] != self.agent_history_length:\n",
    "                    continue\n",
    "                break\n",
    "            self.minibatch_indices[i] = index\n",
    "\n",
    "    # @jit\n",
    "    def get_minibatch(self):\n",
    "        if self.count < self.agent_history_length:\n",
    "            raise ValueError('Not enough memories to get a minibatch')\n",
    "\n",
    "        self._get_valid_indices()\n",
    "\n",
    "        for i, idx in enumerate(self.minibatch_indices):\n",
    "            self.minibatch_states[i] = self._get_state(idx - 1)\n",
    "            self.minibatch_new_states[i] = self._get_state(idx)\n",
    "            if self.use_estimated_reward:\n",
    "                self.minibatch_rewards[i] = self.backfilled_reward[idx] * \\\n",
    "                                            np.power(self.backfill_factor[idx], self.reward_extrapolation_exponent)\n",
    "            else:\n",
    "                self.minibatch_rewards[i] = self.rewards[idx]\n",
    "\n",
    "        return np.transpose(self.minibatch_states, axes=(0, 2, 3, 1)), self.actions[self.minibatch_indices], \\\n",
    "               self.minibatch_rewards, np.transpose(self.minibatch_new_states, axes=(0, 2, 3, 1)), \\\n",
    "               self.terminal_flags[self.minibatch_indices]\n",
    "\n",
    "\n",
    "class SpotlightAttention:\n",
    "\n",
    "    def __init__(self, input_shape):\n",
    "        self.embedding_dimension = 10\n",
    "        self.spotlight_model = self.build_spotlight_model(input_shape, self.embedding_dimension)\n",
    "        self.threshold = .01\n",
    "\n",
    "    def build_spotlight_model(self,input_shape, embedding_dimension):\n",
    "        frames_input = Input(shape=input_shape)\n",
    "        normalized = layers.Lambda(lambda x: x / 255.0, name='norm')(frames_input)\n",
    "        net = Conv2D(8, 8, strides=(4, 4), activation='relu', use_bias=False)(normalized)\n",
    "        net = Conv2D(16, 4, strides=(2, 2), activation='relu', use_bias=False)(net)\n",
    "        net = Conv2D(16, 3, strides=(1, 1), activation='relu', use_bias=False)(net)\n",
    "        net = Flatten()(net)\n",
    "        net = Dense(embedding_dimension * 2, activation='relu', use_bias=False)(net)\n",
    "        net = Dense(embedding_dimension, use_bias=False)(net)\n",
    "        model = Model(inputs=frames_input, outputs=net)\n",
    "        optimizer = RMSprop()\n",
    "        # optimizer = Adam(lr=self.learning_rate)\n",
    "        model.compile(optimizer, loss='mean_squared_error')\n",
    "\n",
    "        return model\n",
    "\n",
    "    def spotlight_train(self, image):\n",
    "        out = np.ones((1,self.embedding_dimension))\n",
    "        history = self.spotlight_model.fit(image, out, epochs=1, verbose=0)\n",
    "\n",
    "    def seen_before(self, image):\n",
    "        res = self.spotlight_model.predict(image)\n",
    "        dist = np.linalg.norm(res-np.ones(self.embedding_dimension))\n",
    "        return dist < self.threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EPISODE_LENGTH= 18000\n",
    "NO_OP_STEPS= 10\n",
    "MAX_EPISODES= 2000\n",
    "AGENT_HISTORY_LENGTH= 4\n",
    "UPDATE_FREQ= 4\n",
    "NETW_UPDATE_FREQ= 10000\n",
    "REPLAY_MEMORY_START_SIZE = 50000\n",
    "DISCOUNT_FACTOR= 0.95\n",
    "MEMORY_SIZE = 1000000\n",
    "BS= 32\n",
    "LEARNING_RATE= 0.0001\n",
    "PUNISH= 1.0\n",
    "INI_EPSILON= 1.0\n",
    "END_EPSILON= 0.1\n",
    "MIN_OBSERVE_EPISODE= 200\n",
    "GAME_ENV= \"BreakoutDeterministic-v4\"\n",
    "REWARD_EXTRAPOLATION_EXPONENT = 2.0\n",
    "frame_height = 84\n",
    "frame_width = 84\n",
    "LINEAR_EXPLORATION_EXPONENT = True\n",
    "USE_DOUBLE_MODEL = True\n",
    "\n",
    "START_EPISODE = 400\n",
    "END_EPISODE = 1800\n",
    "START_EXPONENT = 1.0\n",
    "END_EXPONENT = 40.0\n",
    "IGNORE_EXPONENT_EPISODE = 2000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from player.player import Player\n",
    "# from environments.simulator import Atari\n",
    "import numpy as np\n",
    "import datetime\n",
    "# from utils import HandleResults\n",
    "import numba\n",
    "\n",
    "\n",
    "GAME_ENV = 'BreakoutDeterministic-v4'\n",
    "# GAME_ENV = 'SpaceInvaders-v4' # 758 frames\n",
    "# GAME_ENV = 'Alien-v4' # 948 frames\n",
    "# GAME_ENV = 'Amidar-v4' # 812 frames\n",
    "# GAME_ENV = 'Venture-v4'\n",
    "# GAME_ENV = 'Assault-v4' # 876 frames\n",
    "# GAME_ENV = 'RoadRunner-v4' # 437 frames\n",
    "# GAME_ENV = 'PongDeterministic-v4'\n",
    "# GAME_ENV = 'Asterix-v4'\n",
    "# GAME_ENV = 'MontezumaRevenge-v4'\n",
    "# GAME_ENV = 'ChopperCommand-v4'\n",
    "# OUT_FOLDER = './output/Punish_0_No_Reward_exploration/'\n",
    "# OUT_FOLDER = './output/Punish_1_No_Reward_exploration/'\n",
    "OUT_FOLDER = './output/Punish_1_Reward_exploration_2/'\n",
    "\n",
    "results_handler = HandleResults(GAME_ENV, OUT_FOLDER)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The environment has the following 4 actions: ['NOOP', 'FIRE', 'RIGHT', 'LEFT']\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_17 (InputLayer)           (None, 84, 84, 4)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "norm (Lambda)                   (None, 84, 84, 4)    0           input_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, 20, 20, 32)   8224        norm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, 9, 9, 64)     32832       conv2d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, 7, 7, 64)     36928       conv2d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_17 (Flatten)            (None, 3136)         0           conv2d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_33 (Dense)                (None, 512)          1606144     flatten_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_34 (Dense)                (None, 4)            2052        dense_33[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "action_mask (InputLayer)        (None, 4)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "QValue (Multiply)               (None, 4)            0           dense_34[0][0]                   \n",
      "                                                                 action_mask[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 1,686,180\n",
      "Trainable params: 1,686,180\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_18 (InputLayer)           (None, 84, 84, 4)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "norm (Lambda)                   (None, 84, 84, 4)    0           input_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, 20, 20, 32)   8224        norm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_53 (Conv2D)              (None, 9, 9, 64)     32832       conv2d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_54 (Conv2D)              (None, 7, 7, 64)     36928       conv2d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_18 (Flatten)            (None, 3136)         0           conv2d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_35 (Dense)                (None, 512)          1606144     flatten_18[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_36 (Dense)                (None, 4)            2052        dense_35[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "action_mask (InputLayer)        (None, 4)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "QValue (Multiply)               (None, 4)            0           dense_36[0][0]                   \n",
      "                                                                 action_mask[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 1,686,180\n",
      "Trainable params: 1,686,180\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "MAX_EPISODE_LENGTH :  18000\n",
      "NO_OP_STEPS :  10\n",
      "MAX_EPISODES :  2000\n",
      "AGENT_HISTORY_LENGTH :  4\n",
      "UPDATE_FREQ :  4\n",
      "NETW_UPDATE_FREQ :  10000\n",
      "REPLAY_MEMORY_START_SIZE :  50000\n",
      "DISCOUNT_FACTOR :  0.95\n",
      "MEMORY_SIZE :  1000000\n",
      "BS :  32\n",
      "LEARNING_RATE :  0.0001\n",
      "PUNISH :  1.0\n",
      "INI_EPSILON :  1.0\n",
      "END_EPSILON :  0.1\n",
      "MIN_OBSERVE_EPISODE :  200\n",
      "GAME_ENV :  BreakoutDeterministic-v4\n",
      "REWARD_EXTRAPOLATION_EXPONENT :  2.0\n",
      "LINEAR_EXPLORATION_EXPONENT :  True\n",
      "USE_DOUBLE_MODEL :  True\n",
      "frame_height :  84\n",
      "frame_width :  84\n",
      "****************************\n"
     ]
    }
   ],
   "source": [
    "load_folder=''\n",
    "load_model=False\n",
    "\n",
    "# if load_folder is not '':\n",
    "#     player, game_env, max_episode_length, max_number_of_episodes, all_settings = \\\n",
    "#         results_handler.load_settings(load_folder, load_model)\n",
    "# else:\n",
    "#     player, game_env, max_episode_length, max_number_of_episodes, all_settings = \\\n",
    "#         results_handler.load_default_settings(GAME_ENV)\n",
    "\n",
    "player, game_env, max_episode_length, max_number_of_episodes, all_settings = \\\n",
    "    results_handler.load_default_settings_constants(GAME_ENV)\n",
    "\n",
    "    \n",
    "    \n",
    "for k, v in all_settings.items():\n",
    "    print(k, ': ', v)\n",
    "\n",
    "print('****************************')\n",
    "\n",
    "results_handler.save_settings(all_settings, player)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\skimage\\transform\\_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'time': '0:00:01.563906', 'episode': 0, 'total_frames': 199.0, 'epsilon': '1.000', 'highest_reward': 1.0, 'mean_rewards': 1.0, 'mean_loss': 'nan', 'sparsity': 31.333333333333332}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "res_dict = {}\n",
    "\n",
    "highest_reward = 0\n",
    "total_frames = 0.0\n",
    "prev_frames = 0.0\n",
    "all_rewards = []\n",
    "time = datetime.datetime.now()\n",
    "prev_time = time\n",
    "best_evaluation = 0\n",
    "\n",
    "for episode in range(max_number_of_episodes):\n",
    "    episode_reward, total_frames = run_episode(max_episode_length, episode, game_env, player, total_frames)\n",
    "\n",
    "    # all_rewards[episode] = episode_reward\n",
    "    all_rewards.append(episode_reward)\n",
    "\n",
    "    if episode_reward>highest_reward:\n",
    "        highest_reward = episode_reward\n",
    "\n",
    "    if episode % 10 == 0:\n",
    "        # evaluation_reward, _ = run_episode(max_episode_length, episode, game_env, player, 0, evaluation=True)\n",
    "\n",
    "        # if evaluation_reward > best_evaluation:\n",
    "        #     best_evaluation = evaluation_reward\n",
    "            # print('Best eval: ', str(best_evaluation))\n",
    "\n",
    "        now = datetime.datetime.now()\n",
    "        res_dict['time'] = str(now - time)\n",
    "        res_dict['episode'] = episode\n",
    "        res_dict['total_frames'] = total_frames\n",
    "        res_dict['epsilon'] = format(player.epsilon, '.3f')\n",
    "        res_dict['highest_reward'] = highest_reward\n",
    "        # res_dict['best_eval'] = best_evaluation\n",
    "        res_dict['mean_rewards'] = np.mean(all_rewards[-10:])\n",
    "        res_dict['mean_loss'] = format(np.mean(player.losses[-10:]), '.5f')\n",
    "        # res_dict['memory_vol'] = player.memory.count\n",
    "        # res_dict['fps'] = (total_frames - prev_frames) / ((now - prev_time).total_seconds())\n",
    "        # res_dict['sparsity'] = np.mean(player.memory.sparsity_lengths[-10:])\n",
    "        res_dict['estimating_reward'] = player.memory.use_estimated_reward\n",
    "        res_dict['reward_exponent'] = player.memory.reward_extrapolation_exponent\n",
    "\n",
    "        results_handler.save_res(res_dict)\n",
    "\n",
    "        prev_time = now\n",
    "        prev_frames = total_frames"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
